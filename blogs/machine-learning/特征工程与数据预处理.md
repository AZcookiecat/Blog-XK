# 特征工程与数据预处理

date: 2025-06-24
author: 井上川
techTags: 机器学习, 特征工程, Python
softwareTags: 教程
collection: 机器学习实战指南
summary: 本文详细介绍了机器学习中至关重要的特征工程技术，包括数据理解与探索、数据清洗、特征选择、特征提取、特征变换、特征构造和特征评估等核心步骤，以及各种实用的Python实现方法，帮助读者掌握提高模型性能的关键技能。

## 概述

本文详细介绍了机器学习中至关重要的特征工程技术，包括数据理解与探索、数据清洗、特征选择、特征提取、特征变换、特征构造和特征评估等核心步骤，以及各种实用的Python实现方法，帮助读者掌握提高模型性能的关键技能。

## 什么是特征工程？

在[Python机器学习库Scikit-learn入门](Python机器学习库Scikit-learn入门.md)中，我们介绍了Scikit-learn库的基本使用方法。本文将详细介绍机器学习中至关重要的一环——特征工程。

特征工程（Feature Engineering）是将原始数据转换为更好地代表预测模型的潜在问题的特征的过程，从而提高对未知数据的模型预测准确性。简单来说，特征工程就是从原始数据中提取有意义的特征，使其能够更好地被机器学习算法理解和使用。

## 为什么特征工程很重要？

特征工程是机器学习项目成功的关键因素之一，它的重要性主要体现在以下几个方面：

- **提高模型性能**：良好的特征工程可以显著提高模型的预测准确性
- **减少过拟合风险**：通过选择和构造有意义的特征，可以降低模型的复杂度，减少过拟合风险
- **加速模型训练**：经过处理的特征可以加快模型的收敛速度
- **增强模型可解释性**：合理的特征设计可以使模型的决策过程更加透明
- **解决数据质量问题**：通过数据清洗和预处理，可以处理缺失值、异常值等数据质量问题

## 特征工程的主要步骤

特征工程通常包括以下几个主要步骤：

1. **数据理解与探索**：了解数据的结构、分布和特征之间的关系
2. **数据清洗**：处理缺失值、异常值、重复值等
3. **特征选择**：选择与目标变量相关性高的特征
4. **特征提取**：从原始数据中提取新的特征
5. **特征变换**：对特征进行标准化、归一化、离散化等处理
6. **特征构造**：基于现有特征构造新的特征
7. **特征评估**：评估特征对模型性能的影响

下面我们将详细介绍每个步骤的具体方法和实践。

## 数据理解与探索

在进行特征工程之前，首先需要对数据有深入的理解。数据探索（Exploratory Data Analysis，EDA）是这一步的关键。

### 数据基本信息查看

```python
import pandas as pd
import numpy as np

# 读取数据
df = pd.read_csv('data.csv')

# 查看数据前5行
print(df.head())

# 查看数据基本信息
print(df.info())

# 查看数据统计描述
print(df.describe())

# 查看数据形状
print(f'数据形状: {df.shape}')

# 查看特征类型
print(f'特征类型:\n{df.dtypes}')

# 查看缺失值情况
print(f'缺失值情况:\n{df.isnull().sum()}')
```

### 数据可视化探索

数据可视化是数据探索的重要工具，可以帮助我们直观地了解数据的分布和特征之间的关系。

```python
import matplotlib.pyplot as plt
import seaborn as sns

# 设置中文字体
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号

# 数值型特征直方图
numeric_features = df.select_dtypes(include=['float64', 'int64']).columns
n = len(numeric_features)
fig, axes = plt.subplots(n//3 + (1 if n % 3 else 0), 3, figsize=(15, 4 * (n//3 + (1 if n % 3 else 0))))
axes = axes.flatten()

for i, feature in enumerate(numeric_features):
    sns.histplot(df[feature], kde=True, ax=axes[i])
    axes[i].set_title(f'{feature} 分布')

plt.tight_layout()
plt.show()

# 数值型特征箱线图（检测异常值）
plt.figure(figsize=(15, 10))
sns.boxplot(data=df[numeric_features])
plt.xticks(rotation=90)
plt.title('特征箱线图')
plt.show()

# 相关性热图
plt.figure(figsize=(12, 10))
corr = df[numeric_features].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('特征相关性热图')
plt.show()

# 类别型特征柱状图
categorical_features = df.select_dtypes(include=['object', 'category']).columns
n = len(categorical_features)
fig, axes = plt.subplots(n//2 + (1 if n % 2 else 0), 2, figsize=(15, 5 * (n//2 + (1 if n % 2 else 0))))
axes = axes.flatten()

for i, feature in enumerate(categorical_features):
    sns.countplot(x=feature, data=df, ax=axes[i])
    axes[i].set_title(f'{feature} 分布')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# 特征与目标变量的关系（假设目标变量是target）
if 'target' in df.columns:
    # 数值型特征与目标变量的关系
    for feature in numeric_features:
        if feature != 'target':
            plt.figure(figsize=(10, 6))
            sns.scatterplot(x=feature, y='target', data=df)
            sns.regplot(x=feature, y='target', data=df, scatter=False, color='red')
            plt.title(f'{feature} 与 target 的关系')
            plt.show()

    # 类别型特征与目标变量的关系
    for feature in categorical_features:
        plt.figure(figsize=(10, 6))
        sns.boxplot(x=feature, y='target', data=df)
        plt.title(f'{feature} 与 target 的关系')
        plt.xticks(rotation=45)
        plt.show()
```

## 数据清洗

数据清洗是特征工程的重要步骤，主要包括处理缺失值、异常值、重复值等。

### 处理缺失值

缺失值是数据中常见的问题，处理缺失值的方法主要有以下几种：

#### 1. 删除缺失值

```python
# 删除包含缺失值的行
df_dropna_rows = df.dropna()

# 删除包含缺失值的列
df_dropna_cols = df.dropna(axis=1)

# 删除特定列中包含缺失值的行
df_dropna_specific = df.dropna(subset=['column1', 'column2'])
```

#### 2. 填充缺失值

```python
# 使用0填充所有缺失值
df_fill_zero = df.fillna(0)

# 使用特定值填充缺失值
df_fill_value = df.fillna({'column1': 0, 'column2': 'unknown'})

# 使用均值填充数值型特征的缺失值
numeric_features = df.select_dtypes(include=['float64', 'int64']).columns
df_fill_mean = df.copy()
for feature in numeric_features:
    df_fill_mean[feature] = df_fill_mean[feature].fillna(df_fill_mean[feature].mean())

# 使用中位数填充数值型特征的缺失值
df_fill_median = df.copy()
for feature in numeric_features:
    df_fill_median[feature] = df_fill_median[feature].fillna(df_fill_median[feature].median())

# 使用众数填充类别型特征的缺失值
categorical_features = df.select_dtypes(include=['object', 'category']).columns
df_fill_mode = df.copy()
for feature in categorical_features:
    df_fill_mode[feature] = df_fill_mode[feature].fillna(df_fill_mode[feature].mode()[0])

# 使用前向填充（适用于时间序列数据）
df_fill_ffill = df.fillna(method='ffill')

# 使用后向填充
df_fill_bfill = df.fillna(method='bfill')
```

#### 3. 使用插值法填充缺失值

```python
# 线性插值
df_interpolate_linear = df.interpolate(method='linear')

# 时间插值（适用于时间序列数据）
df_interpolate_time = df.interpolate(method='time')

# 多项式插值
df_interpolate_poly = df.interpolate(method='polynomial', order=2)
```

#### 4. 使用模型预测缺失值

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# 使用迭代插值器（基于随机森林）
imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)

# 对数值型特征进行插值
numeric_data = df[numeric_features]
imputed_data = imputer.fit_transform(numeric_data)

# 将插值后的数据转换回DataFrame
df_imputed = pd.DataFrame(imputed_data, columns=numeric_features, index=numeric_data.index)
```

### 处理异常值

异常值可能会对模型性能产生负面影响，处理异常值的方法主要有以下几种：

#### 1. 基于统计方法检测异常值

```python
# 使用Z-score方法检测异常值
def detect_outliers_zscore(data, threshold=3):
    mean = np.mean(data)
    std = np.std(data)
    z_scores = [(x - mean) / std for x in data]
    outliers = np.abs(z_scores) > threshold
    return outliers

# 对每个数值型特征检测异常值
for feature in numeric_features:
    outliers = detect_outliers_zscore(df[feature])
    print(f'{feature} 中的异常值数量: {outliers.sum()}')

# 使用IQR方法检测异常值
def detect_outliers_iqr(data, factor=1.5):
    q1 = np.percentile(data, 25)
    q3 = np.percentile(data, 75)
    iqr = q3 - q1
    lower_bound = q1 - factor * iqr
    upper_bound = q3 + factor * iqr
    outliers = (data < lower_bound) | (data > upper_bound)
    return outliers, lower_bound, upper_bound

# 对每个数值型特征检测异常值
for feature in numeric_features:
    outliers, lower_bound, upper_bound = detect_outliers_iqr(df[feature])
    print(f'{feature} 中的异常值数量: {outliers.sum()}')
    print(f'下界: {lower_bound}, 上界: {upper_bound}')
```

#### 2. 处理异常值

```python
# 删除异常值
df_no_outliers = df.copy()
for feature in numeric_features:
    outliers, lower_bound, upper_bound = detect_outliers_iqr(df[feature])
    df_no_outliers = df_no_outliers[(df_no_outliers[feature] >= lower_bound) & (df_no_outliers[feature] <= upper_bound)]

# 盖帽法处理异常值（将异常值替换为边界值）
df_winsorized = df.copy()
for feature in numeric_features:
    outliers, lower_bound, upper_bound = detect_outliers_iqr(df[feature])
    df_winsorized[feature] = df_winsorized[feature].clip(lower_bound, upper_bound)

# 使用中位数或均值替换异常值
df_replace_outliers = df.copy()
for feature in numeric_features:
    outliers, lower_bound, upper_bound = detect_outliers_iqr(df[feature])
    # 使用中位数替换
    median = df_replace_outliers[feature].median()
    df_replace_outliers.loc[outliers, feature] = median
```

### 处理重复值

```python
# 检测重复行
print(f'重复行数量: {df.duplicated().sum()}')

# 查看重复行
print(df[df.duplicated()])

# 删除重复行
df_no_duplicates = df.drop_duplicates()
print(f'删除重复行后的数据形状: {df_no_duplicates.shape}')
```

### 处理不一致的数据格式

```python
# 统一日期格式
df['date'] = pd.to_datetime(df['date'], errors='coerce')

# 统一字符串格式（转为小写）
for feature in categorical_features:
    df[feature] = df[feature].str.lower()

# 去除字符串两端的空格
for feature in categorical_features:
    df[feature] = df[feature].str.strip()

# 处理不一致的类别值
# 例如，将 'yes', 'Yes', 'YES' 统一为 'yes'
df['category'] = df['category'].replace({'Yes': 'yes', 'YES': 'yes'})
```

## 特征选择

特征选择是从原始特征中选择最相关的特征子集的过程，它可以减少过拟合风险、提高模型性能、加速模型训练。

### 基于统计的特征选择

#### 1. 皮尔逊相关系数

```python
# 计算特征与目标变量的皮尔逊相关系数
corr_with_target = df[numeric_features].corrwith(df['target'])

# 按绝对值大小排序
corr_with_target_abs = corr_with_target.abs().sort_values(ascending=False)
print(corr_with_target_abs)

# 选择相关性高的特征
selected_features_corr = corr_with_target_abs[corr_with_target_abs > 0.3].index.tolist()
print(f'选择的特征: {selected_features_corr}')
```

#### 2. 卡方检验（适用于分类任务）

```python
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.preprocessing import LabelEncoder

# 准备数据（卡方检验要求特征和目标都是非负整数）
X = df.drop('target', axis=1)
y = df['target']

# 对类别特征进行编码
encoder = LabelEncoder()
for feature in categorical_features:
    if feature != 'target':
        X[feature] = encoder.fit_transform(X[feature])

# 使用卡方检验选择K个最佳特征
selector = SelectKBest(chi2, k=5)
X_new = selector.fit_transform(X, y)

# 获取选择的特征索引
selected_indices = selector.get_support(indices=True)
selected_features_chi2 = X.columns[selected_indices].tolist()
print(f'选择的特征: {selected_features_chi2}')
```

#### 3. F检验（适用于分类和回归任务）

```python
from sklearn.feature_selection import SelectKBest, f_classif, f_regression

# 对于分类任务
selector = SelectKBest(f_classif, k=5)
X_new = selector.fit_transform(X, y)

# 获取选择的特征索引
selected_indices = selector.get_support(indices=True)
selected_features_fclassif = X.columns[selected_indices].tolist()
print(f'选择的特征: {selected_features_fclassif}')

# 对于回归任务
selector = SelectKBest(f_regression, k=5)
X_new = selector.fit_transform(X, y)

# 获取选择的特征索引
selected_indices = selector.get_support(indices=True)
selected_features_fregression = X.columns[selected_indices].tolist()
print(f'选择的特征: {selected_features_fregression}')
```

### 基于模型的特征选择

#### 1. 递归特征消除（RFE）

```python
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# 创建RFE选择器
estimator = RandomForestClassifier(n_estimators=100, random_state=42)
rfe = RFE(estimator, n_features_to_select=5, step=1)

# 拟合模型
rfe.fit(X, y)

# 获取选择的特征
selected_features_rfe = X.columns[rfe.support_].tolist()
print(f'选择的特征: {selected_features_rfe}')

# 查看特征排名
feature_ranking = pd.DataFrame({'Feature': X.columns, 'Ranking': rfe.ranking_})
feature_ranking = feature_ranking.sort_values('Ranking')
print(feature_ranking)
```

#### 2. 基于树模型的特征重要性

```python
from sklearn.ensemble import RandomForestClassifier

# 训练随机森林模型
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

# 获取特征重要性
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': rf.feature_importances_})
feature_importance = feature_importance.sort_values('Importance', ascending=False)
print(feature_importance)

# 可视化特征重要性
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance)
plt.title('特征重要性')
plt.show()

# 选择重要性高的特征
selected_features_importance = feature_importance[feature_importance['Importance'] > 0.05]['Feature'].tolist()
print(f'选择的特征: {selected_features_importance}')
```

### 基于正则化的特征选择

```python
from sklearn.linear_model import Lasso, Ridge
from sklearn.preprocessing import StandardScaler

# 标准化特征（Lasso和Ridge对特征尺度敏感）
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 使用Lasso进行特征选择
lasso = Lasso(alpha=0.1, random_state=42)
lasso.fit(X_scaled, y)

# 获取特征系数
lasso_coef = pd.DataFrame({'Feature': X.columns, 'Coefficient': lasso.coef_})
lasso_coef = lasso_coef.sort_values('Coefficient', ascending=False)
print(lasso_coef)

# 选择系数非零的特征
selected_features_lasso = lasso_coef[lasso_coef['Coefficient'] != 0]['Feature'].tolist()
print(f'选择的特征: {selected_features_lasso}')

# 使用Ridge进行特征选择
ridge = Ridge(alpha=1.0, random_state=42)
ridge.fit(X_scaled, y)

# 获取特征系数
ridge_coef = pd.DataFrame({'Feature': X.columns, 'Coefficient': ridge.coef_})
ridge_coef = ridge_coef.sort_values('Coefficient', ascending=False)
print(ridge_coef)
```

## 特征提取

特征提取是从原始数据中提取新的特征的过程，主要用于处理高维数据或复杂数据。

### 主成分分析（PCA）

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 标准化特征
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 创建PCA模型
pca = PCA(n_components=5)  # 提取5个主成分

# 拟合并转换数据
X_pca = pca.fit_transform(X_scaled)

# 查看解释方差比
print(f'解释方差比: {pca.explained_variance_ratio_}')
print(f'累计解释方差比: {sum(pca.explained_variance_ratio_):.4f}')

# 将PCA结果转换为DataFrame
pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]
df_pca = pd.DataFrame(X_pca, columns=pca_columns, index=X.index)

# 可视化PCA结果
plt.figure(figsize=(10, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.xlabel(f'主成分1 ({pca.explained_variance_ratio_[0]:.2%})')
plt.ylabel(f'主成分2 ({pca.explained_variance_ratio_[1]:.2%})')
plt.title('PCA降维结果')
plt.colorbar(label='Target')
plt.show()
```

### 线性判别分析（LDA）

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# 创建LDA模型
lda = LinearDiscriminantAnalysis(n_components=2)  # LDA最多只能提取类别数-1个特征

# 拟合并转换数据
X_lda = lda.fit_transform(X_scaled, y)

# 查看解释方差比
print(f'解释方差比: {lda.explained_variance_ratio_}')
print(f'累计解释方差比: {sum(lda.explained_variance_ratio_):.4f}')

# 将LDA结果转换为DataFrame
lda_columns = [f'LD{i+1}' for i in range(X_lda.shape[1])]
df_lda = pd.DataFrame(X_lda, columns=lda_columns, index=X.index)

# 可视化LDA结果
plt.figure(figsize=(10, 6))
plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.xlabel(f'线性判别1 ({lda.explained_variance_ratio_[0]:.2%})')
plt.ylabel(f'线性判别2 ({lda.explained_variance_ratio_[1]:.2%})')
plt.title('LDA降维结果')
plt.colorbar(label='Target')
plt.show()
```

### t-SNE降维

```python
from sklearn.manifold import TSNE

# 创建t-SNE模型
tsne = TSNE(n_components=2, random_state=42, perplexity=30)

# 转换数据
X_tsne = tsne.fit_transform(X_scaled)

# 将t-SNE结果转换为DataFrame
tsne_columns = [f'tSNE{i+1}' for i in range(X_tsne.shape[1])]
df_tsne = pd.DataFrame(X_tsne, columns=tsne_columns, index=X.index)

# 可视化t-SNE结果
plt.figure(figsize=(10, 6))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.title('t-SNE降维结果')
plt.colorbar(label='Target')
plt.show()
```

### 独立成分分析（ICA）

```python
from sklearn.decomposition import FastICA

# 创建ICA模型
ica = FastICA(n_components=5, random_state=42)

# 拟合并转换数据
X_ica = ica.fit_transform(X_scaled)

# 将ICA结果转换为DataFrame
ica_columns = [f'IC{i+1}' for i in range(X_ica.shape[1])]
df_ica = pd.DataFrame(X_ica, columns=ica_columns, index=X.index)
```

## 特征变换

特征变换是对特征进行标准化、归一化、离散化等处理的过程，它可以使特征更适合机器学习算法的要求。

### 数值型特征变换

#### 1. 标准化（Standardization）

```python
from sklearn.preprocessing import StandardScaler

# 创建标准化器
scaler = StandardScaler()

# 拟合并转换数据
X_scaled = scaler.fit_transform(X[numeric_features])

# 将标准化后的数据转换回DataFrame
df_scaled = pd.DataFrame(X_scaled, columns=numeric_features, index=X.index)

# 查看标准化后的数据统计描述
print(df_scaled.describe())
```

#### 2. 归一化（Normalization）

```python
from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, RobustScaler

# MinMaxScaler（缩放到[0, 1]范围）
min_max_scaler = MinMaxScaler()
X_minmax = min_max_scaler.fit_transform(X[numeric_features])
df_minmax = pd.DataFrame(X_minmax, columns=numeric_features, index=X.index)

# MaxAbsScaler（缩放到[-1, 1]范围）
max_abs_scaler = MaxAbsScaler()
X_maxabs = max_abs_scaler.fit_transform(X[numeric_features])
df_maxabs = pd.DataFrame(X_maxabs, columns=numeric_features, index=X.index)

# RobustScaler（基于中位数和IQR，对异常值不敏感）
robust_scaler = RobustScaler()
X_robust = robust_scaler.fit_transform(X[numeric_features])
df_robust = pd.DataFrame(X_robust, columns=numeric_features, index=X.index)
```

#### 3. 幂变换（Power Transformation）

```python
from sklearn.preprocessing import PowerTransformer, QuantileTransformer

# Box-Cox变换（要求特征为正）
box_cox = PowerTransformer(method='box-cox', standardize=True)
X_boxcox = box_cox.fit_transform(X[numeric_features][X[numeric_features] > 0])
df_boxcox = pd.DataFrame(X_boxcox, columns=numeric_features, index=X[numeric_features][X[numeric_features] > 0].index)

# Yeo-Johnson变换（可以处理零和负值）
yeo_johnson = PowerTransformer(method='yeo-johnson', standardize=True)
X_yeojohnson = yeo_johnson.fit_transform(X[numeric_features])
df_yeojohnson = pd.DataFrame(X_yeojohnson, columns=numeric_features, index=X.index)

# 分位数变换（将特征映射到均匀分布）
quantile_uniform = QuantileTransformer(output_distribution='uniform', random_state=42)
X_quantile_uniform = quantile_uniform.fit_transform(X[numeric_features])
df_quantile_uniform = pd.DataFrame(X_quantile_uniform, columns=numeric_features, index=X.index)

# 分位数变换（将特征映射到正态分布）
quantile_normal = QuantileTransformer(output_distribution='normal', random_state=42)
X_quantile_normal = quantile_normal.fit_transform(X[numeric_features])
df_quantile_normal = pd.DataFrame(X_quantile_normal, columns=numeric_features, index=X.index)
```

#### 4. 离散化（Binarization and Discretization）

```python
from sklearn.preprocessing import Binarizer, KBinsDiscretizer

# 二值化（将特征转换为0或1）
binarizer = Binarizer(threshold=0.5)
X_binarized = binarizer.fit_transform(X[numeric_features])
df_binarized = pd.DataFrame(X_binarized, columns=[f'{col}_binary' for col in numeric_features], index=X.index)

# 分箱离散化（将特征转换为多个离散区间）
kbins = KBinsDiscretizer(n_bins=5, encode='onehot-dense', strategy='quantile')
X_kbins = kbins.fit_transform(X[numeric_features])

# 创建分箱后的列名
bin_columns = []
for i, feature in enumerate(numeric_features):
    for j in range(kbins.n_bins_[i]):
        bin_columns.append(f'{feature}_bin{j+1}')

df_kbins = pd.DataFrame(X_kbins, columns=bin_columns, index=X.index)

# 手动分箱
df_discretized = df.copy()
for feature in numeric_features:
    # 等宽分箱
    df_discretized[f'{feature}_equal_width'] = pd.cut(df_discretized[feature], bins=5, labels=False)
    # 等频分箱
    df_discretized[f'{feature}_equal_freq'] = pd.qcut(df_discretized[feature], q=5, labels=False, duplicates='drop')
```

### 类别型特征变换

#### 1. 标签编码（Label Encoding）

```python
from sklearn.preprocessing import LabelEncoder

# 创建标签编码器
label_encoder = LabelEncoder()

df_label_encoded = df.copy()
for feature in categorical_features:
    if feature != 'target':
        df_label_encoded[feature] = label_encoder.fit_transform(df_label_encoded[feature])
```

#### 2. 独热编码（One-Hot Encoding）

```python
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# 使用ColumnTransformer进行独热编码
ct = ColumnTransformer(
    transformers=[
        ('onehot', OneHotEncoder(sparse=False, drop='first'), categorical_features)
    ],
    remainder='passthrough'
)

X_encoded = ct.fit_transform(df)

# 获取编码后的列名
onehot_columns = ct.named_transformers_['onehot'].get_feature_names_out(categorical_features)
all_columns = list(onehot_columns) + [col for col in df.columns if col not in categorical_features]

df_encoded = pd.DataFrame(X_encoded, columns=all_columns, index=df.index)

# 使用pandas进行独热编码
df_onehot = pd.get_dummies(df, columns=categorical_features, drop_first=True)
```

#### 3. 目标编码（Target Encoding）

```python
from category_encoders import TargetEncoder

# 创建目标编码器
target_encoder = TargetEncoder(cols=categorical_features, smoothing=0.2)

# 拟合并转换数据
df_target_encoded = target_encoder.fit_transform(df, df['target'])

# 手动实现目标编码
df_manual_target_encoded = df.copy()
for feature in categorical_features:
    if feature != 'target':
        # 计算每个类别值的目标变量均值
        target_mean = df.groupby(feature)['target'].mean()
        # 用均值替换类别值
        df_manual_target_encoded[feature] = df_manual_target_encoded[feature].map(target_mean)
```

#### 4. 计数编码（Count Encoding）

```python
# 手动实现计数编码
df_count_encoded = df.copy()
for feature in categorical_features:
    if feature != 'target':
        # 计算每个类别值的出现次数
        count = df[feature].value_counts()
        # 用计数替换类别值
        df_count_encoded[feature] = df_count_encoded[feature].map(count)
```

#### 5. 频率编码（Frequency Encoding）

```python
# 手动实现频率编码
df_freq_encoded = df.copy()
for feature in categorical_features:
    if feature != 'target':
        # 计算每个类别值的出现频率
        freq = df[feature].value_counts(normalize=True)
        # 用频率替换类别值
        df_freq_encoded[feature] = df_freq_encoded[feature].map(freq)
```

## 特征构造

特征构造是基于现有特征构造新的特征的过程，它可以提取数据中的深层信息，提高模型的预测能力。

### 数学运算特征构造

```python
# 创建新的数学运算特征
df_feature_engineering = df.copy()

# 特征相加
df_feature_engineering['feature_sum'] = df['feature1'] + df['feature2']

# 特征相减
df_feature_engineering['feature_diff'] = df['feature1'] - df['feature2']

# 特征相乘
df_feature_engineering['feature_product'] = df['feature1'] * df['feature2']

# 特征相除（避免除以0）
df_feature_engineering['feature_ratio'] = df['feature1'] / (df['feature2'] + 1e-8)

# 特征平方
df_feature_engineering['feature_squared'] = df['feature1'] ** 2

# 特征平方根（确保非负）
df_feature_engineering['feature_sqrt'] = np.sqrt(np.maximum(0, df['feature1']))

# 特征对数（确保正数）
df_feature_engineering['feature_log'] = np.log(np.maximum(1e-8, df['feature1']))

# 特征指数
df_feature_engineering['feature_exp'] = np.exp(df['feature1'])

# 特征绝对值
df_feature_engineering['feature_abs'] = np.abs(df['feature1'])
```

### 统计特征构造

```python
# 创建基于统计的特征
df_stat_features = df.copy()

# 计算多个特征的统计量
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
if 'target' in numeric_cols:
    numeric_cols.remove('target')

# 均值特征
df_stat_features['numeric_mean'] = df[numeric_cols].mean(axis=1)

# 中位数特征
df_stat_features['numeric_median'] = df[numeric_cols].median(axis=1)

# 标准差特征
df_stat_features['numeric_std'] = df[numeric_cols].std(axis=1)

# 最大值特征
df_stat_features['numeric_max'] = df[numeric_cols].max(axis=1)

# 最小值特征
df_stat_features['numeric_min'] = df[numeric_cols].min(axis=1)

# 范围特征（最大值-最小值）
df_stat_features['numeric_range'] = df_stat_features['numeric_max'] - df_stat_features['numeric_min']

# 偏度特征
df_stat_features['numeric_skew'] = df[numeric_cols].skew(axis=1)

# 峰度特征
df_stat_features['numeric_kurtosis'] = df[numeric_cols].kurtosis(axis=1)
```

### 时间特征构造

```python
# 假设数据中包含时间列
df_time_features = df.copy()
df_time_features['timestamp'] = pd.to_datetime(df_time_features['timestamp'])

# 提取年份
df_time_features['year'] = df_time_features['timestamp'].dt.year

# 提取月份
df_time_features['month'] = df_time_features['timestamp'].dt.month

# 提取日期
df_time_features['day'] = df_time_features['timestamp'].dt.day

# 提取星期几
df_time_features['day_of_week'] = df_time_features['timestamp'].dt.dayofweek

# 提取小时
df_time_features['hour'] = df_time_features['timestamp'].dt.hour

# 提取分钟
df_time_features['minute'] = df_time_features['timestamp'].dt.minute

# 提取是否为周末
df_time_features['is_weekend'] = df_time_features['day_of_week'].isin([5, 6]).astype(int)

# 提取季节
def get_season(month):
    if month in [12, 1, 2]:
        return 'winter'
    elif month in [3, 4, 5]:
        return 'spring'
    elif month in [6, 7, 8]:
        return 'summer'
    else:
        return 'autumn'

df_time_features['season'] = df_time_features['month'].apply(get_season)

# 计算时间差
df_time_features['time_diff'] = (df_time_features['timestamp'] - df_time_features['timestamp'].min()).dt.days

# 提取节假日信息（需要holidays库）
# pip install holidays
import holidays

country_holidays = holidays.CountryHoliday('CN')  # 中国节假日
df_time_features['is_holiday'] = df_time_features['timestamp'].apply(lambda x: x in country_holidays).astype(int)
```

### 文本特征构造

```python
# 假设数据中包含文本列
df_text_features = df.copy()

# 文本长度
df_text_features['text_length'] = df_text_features['text'].str.len()

# 单词数量
df_text_features['word_count'] = df_text_features['text'].str.split().str.len()

# 平均单词长度
def avg_word_length(text):
    words = text.split()
    if len(words) == 0:
        return 0
    return sum(len(word) for word in words) / len(words)

df_text_features['avg_word_length'] = df_text_features['text'].apply(avg_word_length)

# 大写字母数量
df_text_features['uppercase_count'] = df_text_features['text'].str.count(r'[A-Z]')

# 小写字母数量
df_text_features['lowercase_count'] = df_text_features['text'].str.count(r'[a-z]')

# 数字数量
df_text_features['digit_count'] = df_text_features['text'].str.count(r'\d')

# 特殊字符数量
df_text_features['special_char_count'] = df_text_features['text'].str.count(r'[^A-Za-z0-9\s]')

# 标点符号数量
df_text_features['punctuation_count'] = df_text_features['text'].str.count(r'[\p{P}]', regex=True)

# 停用词数量（需要nltk库）
# import nltk
# nltk.download('stopwords')
# from nltk.corpus import stopwords
# stop_words = set(stopwords.words('english'))
# def count_stopwords(text):
#     words = text.lower().split()
#     return sum(1 for word in words if word in stop_words)
# df_text_features['stopword_count'] = df_text_features['text'].apply(count_stopwords)
```

### 组合特征构造

```python
# 创建组合特征
df_combo_features = df.copy()

# 特征交叉（适用于类别特征）
for i, feature1 in enumerate(categorical_features):
    if feature1 == 'target':
        continue
    for feature2 in categorical_features[i+1:]:
        if feature2 == 'target':
            continue
        # 创建新的组合特征
        combo_feature = f'{feature1}_{feature2}'
        df_combo_features[combo_feature] = df_combo_features[feature1].astype(str) + '_' + df_combo_features[feature2].astype(str)

# 数值特征与类别特征的组合
for cat_feature in categorical_features:
    if cat_feature == 'target':
        continue
    for num_feature in numeric_features:
        # 计算每个类别下数值特征的统计量
        grouped_stats = df.groupby(cat_feature)[num_feature].agg(['mean', 'median', 'std', 'min', 'max'])
        grouped_stats.columns = [f'{cat_feature}_{num_feature}_{stat}' for stat in grouped_stats.columns]
        
        # 合并统计量到原始数据
        df_combo_features = df_combo_features.merge(grouped_stats, left_on=cat_feature, right_index=True, how='left')
```

## 特征评估

特征评估是评估特征对模型性能影响的过程，它可以帮助我们确定哪些特征对模型最重要，哪些特征可以删除。

### 基于模型的特征评估

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 训练基础模型（使用所有特征）
base_model = RandomForestClassifier(n_estimators=100, random_state=42)
base_model.fit(X_train, y_train)
base_score = accuracy_score(y_test, base_model.predict(X_test))
print(f'基础模型准确率: {base_score:.4f}')

# 单特征重要性评估
feature_importance_scores = []
for feature in X.columns:
    # 仅使用当前特征训练模型
    single_feature_model = RandomForestClassifier(n_estimators=100, random_state=42)
    single_feature_model.fit(X_train[[feature]], y_train)
    single_feature_score = accuracy_score(y_test, single_feature_model.predict(X_test[[feature]]))
    feature_importance_scores.append((feature, single_feature_score))

# 按性能排序
feature_importance_scores.sort(key=lambda x: x[1], reverse=True)
print('单特征模型性能:')
for feature, score in feature_importance_scores:
    print(f'{feature}: {score:.4f}')

# 特征排除法评估
feature_exclusion_scores = []
for feature in X.columns:
    # 排除当前特征训练模型
    excluded_features = [col for col in X.columns if col != feature]
    exclusion_model = RandomForestClassifier(n_estimators=100, random_state=42)
    exclusion_model.fit(X_train[excluded_features], y_train)
    exclusion_score = accuracy_score(y_test, exclusion_model.predict(X_test[excluded_features]))
    # 计算性能下降（如果为负，表示排除该特征后性能提高）
    performance_drop = base_score - exclusion_score
    feature_exclusion_scores.append((feature, performance_drop))

# 按性能下降程度排序（正值表示该特征重要）
feature_exclusion_scores.sort(key=lambda x: x[1], reverse=True)
print('特征排除法评估:')
for feature, drop in feature_exclusion_scores:
    print(f'{feature}: {drop:.4f}')
```

### 基于排列的特征重要性

```python
from sklearn.inspection import permutation_importance

# 训练模型
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 计算排列重要性
perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)

# 整理排列重要性结果
perm_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': perm_importance.importances_mean,
    'Std': perm_importance.importances_std
})

# 按重要性排序
perm_importance_df = perm_importance_df.sort_values('Importance', ascending=False)
print(perm_importance_df)

# 可视化排列重要性
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=perm_importance_df)
plt.title('排列特征重要性')
plt.show()
```

### 基于SHAP值的特征重要性

```python
# SHAP库可以提供更精确的特征重要性解释
# 需要安装SHAP库: pip install shap
import shap

# 创建解释器
explainer = shap.TreeExplainer(model)

# 计算SHAP值
shap_values = explainer.shap_values(X_test)

# 汇总SHAP值（对于分类任务，需要指定类别）
if len(shap_values) > 1:  # 多类别分类
    shap_summary = np.abs(shap_values).mean(0).mean(0)
else:  # 二分类或回归
    shap_summary = np.abs(shap_values).mean(0)

# 整理SHAP重要性结果
shap_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'SHAP_Importance': shap_summary
})

# 按重要性排序
shap_importance_df = shap_importance_df.sort_values('SHAP_Importance', ascending=False)
print(shap_importance_df)

# 可视化SHAP摘要图
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values, X_test, feature_names=X.columns)
plt.show()

# 可视化单个特征的SHAP依赖图
feature = 'feature1'  # 选择一个特征
plt.figure(figsize=(10, 6))
shap.dependence_plot(feature, shap_values, X_test, feature_names=X.columns)
plt.show()
```

## 特征工程最佳实践

1. **理解业务场景**：特征工程应该基于对业务场景的深入理解，提取与问题相关的特征
2. **数据质量优先**：确保数据质量是特征工程的前提，处理好缺失值、异常值等问题
3. **特征选择与降维结合**：对于高维数据，可以结合使用特征选择和降维技术
4. **自动化特征工程**：对于大型数据集，可以使用自动化特征工程工具（如Featuretools）
5. **持续迭代优化**：特征工程是一个持续迭代的过程，需要根据模型性能不断优化
6. **保留原始特征**：在创建新特征的同时，保留原始特征，避免信息丢失
7. **注意过拟合风险**：过度的特征工程可能导致过拟合，特别是当数据量较小时
8. **考虑特征的可解释性**：选择和构造具有实际业务意义的特征，提高模型的可解释性

## 总结

本文详细介绍了特征工程的各个步骤和常用技术，包括数据理解与探索、数据清洗、特征选择、特征提取、特征变换和特征构造等。特征工程是机器学习项目成功的关键因素之一，良好的特征工程可以显著提高模型的预测性能。

特征工程是一个需要经验和创造力的过程，没有放之四海而皆准的方法。在实际项目中，需要根据数据特点、业务场景和模型需求，选择合适的特征工程技术，并不断迭代优化。

在接下来的文章中，我们将介绍集成学习、深度学习等高级机器学习主题，帮助你进一步提升机器学习能力。

上一篇：[Python机器学习库Scikit-learn入门](Python机器学习库Scikit-learn入门.md)
下一篇：[集成学习与模型融合技术](集成学习与模型融合技术.md)