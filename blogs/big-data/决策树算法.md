# 决策树算法
pinned:  false
author: 井上川
date: 2025-06-24
techTags: Python, 大数据, 决策树算法
softwareTags: 教程
collection: 大数据实战指南
summary: 本文详细介绍了决策树算法的基本原理、构造步骤、特征选取标准（信息增益、增益率、基尼指数）以及剪枝策略，并简要介绍了基于决策树的集成算法随机森林，帮助读者理解这一常用的机器学习算法。

## 概述

本文详细介绍了决策树算法的基本原理、构造步骤、特征选取标准以及剪枝策略，并简要介绍了基于决策树的集成算法随机森林，帮助读者理解这一常用的机器学习算法。

决策树采用自顶向下的递归方法，基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处熵值为0。决策树思想，就是寻找最纯净的划分方法，最纯净在数学上叫纯度。另一种理解就是不纯度。不纯度的选取有很多方法，每种方法也就形成了不同的决策树方法：
-ID3算法使用信息增益作为不纯度
-C4.5算法使用信息增益率作为不纯度
-CART算法使用基尼系数作为不纯度

<div class="tip-box" style="background-color: #f0f8ff; border-left: 4px solid #3498db; padding: 10px; margin: 10px 0;">
  <strong>决策树构造步骤：</strong>节点特征的选择->决策树的构建->决策树修剪（防过拟合）
</div>

## 特征选取
熵是度量样本集合纯度常用的指标，也是表示随机变量不确定性的度量

### 特征选取标准1：信息增益
ID3算法的核心是在决策树各个节点上应用信息增益准则作为选择特征，递归的构建决策树

### 特征选取标准2：增益率

### 特征选取标准3：基尼指数

差别分类和回归的依据是：待预测分类是离散（分类）还是连续（回归）数据

-分类选取依据：具有最小基尼指数的属性及属性值作为分裂标准。基尼指数越小，说明二分之后的子样本纯度越高，说明效果越好
-回归选取依据：具有最小总方差的属性及属性值作为分裂标准。总方差越小，说明二分之后的子样本的“差异性越小”，效果越好

## 决策树剪枝算法
将复杂决策树进行简化，去掉一些节点解决过拟合问题，称为剪枝

**剪枝策略：预剪枝，后剪枝**
	预剪枝：边建立决策树边进行剪枝的操作，更实用
	-预剪枝会限制深度，叶节点个数，叶子节点样本数，信息增量等
	后剪枝；当建立完决策树后再进行剪枝操作
	-通过一定的衡量标准（损失函数），叶子节点越多，损失越大，剪完后损失大就不减，损失小就减


## 随机森林
+以决策树为基分类器的集成算法，通过组合多颗独立的决策树后根据投票或取均值的方式得到最终预测结果的机器学习方法，往往比单颗树具有更高的准确率和更强的稳定性。主要取决于随机抽取样本特征和集成算法，前者让它具有更稳定的抗过拟合能力，后者让它具有更高的准确率

+主要应用于回归和分类。随机森林进行bootstrap抽样，但随机与bagging区别是：生成每棵树时每个节点变量都仅在随机选出的少数变量中产生。因此不但样本是随机的，连每个节点变量的产生也是随机的。


+优点：

+缺点：
在某些噪音比较大的样本集上，RF模型容易陷入过拟合
取值划分比较多的特诊容易对RF决策产生更大的影响，从而影响拟合模型效果

