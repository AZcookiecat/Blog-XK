# 高级数据分析与机器学习应用

date: 2025-06-24
author: 井上川
techTags: 数据分析, 机器学习, 统计分析, 数据建模
softwareTags: 教程, 实战
collection: 数据分析实战指南
summary: 本文详细介绍了高级数据分析技术和机器学习算法在实际业务中的应用，包括高级统计分析方法、机器学习模型、特征工程、模型评估与优化以及多个行业的实战案例，帮助读者提升数据分析能力和解决复杂业务问题的能力。

## 概述

随着数据量的爆炸式增长和计算能力的提升，高级数据分析技术和机器学习算法在各个行业的应用越来越广泛。本文将详细介绍高级数据分析技术和机器学习算法在实际业务中的应用，包括高级统计分析方法、机器学习模型、特征工程、模型评估与优化以及多个行业的实战案例，帮助读者提升数据分析能力和解决复杂业务问题的能力。

## 第一章 高级统计分析方法

### 1.1 高级描述性统计

除了基础的描述性统计指标（均值、中位数、标准差等），高级描述性统计方法可以帮助我们更深入地了解数据的分布和特征。

#### 1.1.1 分布分析

分布分析是研究数据的分布特征和规律的统计方法。

**常见的分布类型**：
- 正态分布（Normal Distribution）
- 二项分布（Binomial Distribution）
- 泊松分布（Poisson Distribution）
- 指数分布（Exponential Distribution）
- 均匀分布（Uniform Distribution）

**分布检验方法**：
- 直方图和密度图分析
- Q-Q图和P-P图分析
- 正态性检验（Shapiro-Wilk检验、Kolmogorov-Smirnov检验）

**Python实现**：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# 生成正态分布数据
data = np.random.normal(0, 1, 1000)

# 绘制直方图和密度图
sns.histplot(data, kde=True)
plt.title('Normal Distribution')
plt.show()

# 绘制Q-Q图
stats.probplot(data, dist="norm", plot=plt)
plt.title('Q-Q Plot')
plt.show()

# 执行Shapiro-Wilk正态性检验
stat, p = stats.shapiro(data)
print(f'Shapiro-Wilk Test: statistic={stat:.4f}, p-value={p:.4f}')
if p > 0.05:
    print('数据符合正态分布')
else:
    print('数据不符合正态分布')
```

#### 1.1.2 相关分析

相关分析是研究变量之间相关关系的统计方法。

**常见的相关系数**：
- Pearson相关系数：衡量线性相关关系
- Spearman相关系数：衡量秩相关关系
- Kendall相关系数：衡量一致性程度

**Python实现**：

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 创建示例数据
df = pd.DataFrame({
    'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'y1': [2, 4, 5, 7, 8, 9, 10, 12, 13, 15],
    'y2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],
    'y3': [3, 5, 2, 8, 4, 10, 7, 1, 9, 6]
})

# 计算相关系数
corr_matrix = df.corr(method='pearson')
print("Pearson相关系数矩阵:")
print(corr_matrix)

# 绘制热力图
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)
plt.title('Correlation Matrix')
plt.show()

# 计算Spearman相关系数
spearman_corr = df.corr(method='spearman')
print("Spearman相关系数矩阵:")
print(spearman_corr)
```

### 1.2 推断性统计分析

推断性统计分析是通过样本数据推断总体特征的统计方法。

#### 1.2.1 假设检验

假设检验是推断性统计的核心方法，用于检验关于总体参数的假设是否成立。

**常见的假设检验**：
- t检验：比较两个样本的均值是否有显著差异
- 卡方检验：检验分类变量之间是否独立
- ANOVA（方差分析）：比较多个样本的均值是否有显著差异
- 回归分析：分析变量之间的因果关系

**Python实现**：

```python
import numpy as np
import pandas as pd
from scipy import stats

# 示例：独立样本t检验
group1 = np.random.normal(50, 10, 100)
group2 = np.random.normal(55, 10, 100)

# 执行t检验
t_stat, p_value = stats.ttest_ind(group1, group2)
print(f'独立样本t检验: t统计量={t_stat:.4f}, p值={p_value:.4f}')
if p_value < 0.05:
    print('两组均值存在显著差异')
else:
    print('两组均值不存在显著差异')

# 示例：卡方检验
data = pd.DataFrame({
    '性别': ['男', '男', '女', '女', '男', '女', '男', '女'],
    '购买决策': ['购买', '不购买', '购买', '不购买', '不购买', '购买', '购买', '不购买']
})

# 创建列联表
contingency_table = pd.crosstab(data['性别'], data['购买决策'])
print("列联表:")
print(contingency_table)

# 执行卡方检验
chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)
print(f'卡方检验: 卡方统计量={chi2_stat:.4f}, p值={p_value:.4f}, 自由度={dof}')
if p_value < 0.05:
    print('性别与购买决策之间存在显著关联')
else:
    print('性别与购买决策之间不存在显著关联')
```

#### 1.2.2 方差分析

方差分析（ANOVA）是用于比较多个（两个以上）样本均值是否有显著差异的统计方法。

**类型**：
- 单因素方差分析：只有一个自变量
- 双因素方差分析：有两个自变量
- 多因素方差分析：有多个自变量

**Python实现**：

```python
import numpy as np
import pandas as pd
from scipy import stats
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

# 示例：单因素方差分析
group1 = np.random.normal(50, 10, 100)
group2 = np.random.normal(55, 10, 100)
group3 = np.random.normal(60, 10, 100)

# 创建数据框
df = pd.DataFrame({
    'value': np.concatenate([group1, group2, group3]),
    'group': ['A']*100 + ['B']*100 + ['C']*100
})

# 使用scipy进行方差分析
f_stat, p_value = stats.f_oneway(group1, group2, group3)
print(f'单因素方差分析: F统计量={f_stat:.4f}, p值={p_value:.4f}')
if p_value < 0.05:
    print('至少有一组均值与其他组存在显著差异')
else:
    print('所有组均值之间不存在显著差异')

# 使用statsmodels进行更详细的方差分析
model = ols('value ~ C(group)', data=df).fit()
anova_table = anova_lm(model)
print("ANOVA表:")
print(anova_table)
```

### 1.3 时间序列分析

时间序列分析是研究随时间变化的数据的统计方法。

#### 1.3.1 时间序列的组成成分

时间序列通常包含以下组成成分：
- 趋势（Trend）：长期的变化方向
- 季节性（Seasonality）：周期性的波动
- 周期性（Cyclicity）：非固定周期的波动
- 随机性（Irregularity）：随机的、不可预测的波动

#### 1.3.2 时间序列分析方法

**常见的时间序列分析方法**：
- 移动平均（Moving Average）
- 指数平滑（Exponential Smoothing）
- ARIMA模型（自回归整合移动平均模型）
- SARIMA模型（季节性ARIMA模型）
-  Prophet模型（Facebook开发的时间序列预测工具）

**Python实现**：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from prophet import Prophet

# 生成示例时间序列数据
dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')
values = np.sin(np.linspace(0, 100, len(dates))) + 0.1 * np.random.randn(len(dates)) + np.linspace(0, 5, len(dates))
ts = pd.Series(values, index=dates)

# 绘制时间序列
plt.figure(figsize=(12, 6))
plt.plot(ts)
plt.title('Time Series Data')
plt.xlabel('Date')
plt.ylabel('Value')
plt.show()

# 时间序列分解
decomposition = seasonal_decompose(ts, model='additive', period=365)
fig = decomposition.plot()
fig.set_size_inches(12, 8)
plt.show()

# ARIMA模型预测
# 注意：实际应用中需要进行模型选择和参数调优
train_size = int(len(ts) * 0.8)
train, test = ts[:train_size], ts[train_size:]

# 拟合ARIMA模型
model = ARIMA(train, order=(1, 1, 1))
model_fit = model.fit()

# 预测
forecast = model_fit.forecast(steps=len(test))

# 评估预测效果
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(test, forecast)
print(f'ARIMA模型MSE: {mse:.4f}')

# 绘制预测结果
plt.figure(figsize=(12, 6))
plt.plot(train, label='Training Data')
plt.plot(test, label='Test Data')
plt.plot(test.index, forecast, label='Forecast')
plt.title('ARIMA Forecast')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend()
plt.show()

# 使用Prophet进行预测
df_prophet = pd.DataFrame({'ds': ts.index, 'y': ts.values})
model = Prophet(daily_seasonality=True)
model.fit(df_prophet)

# 创建未来日期
dfuture = model.make_future_dataframe(periods=365)
forecast = model.predict(dfuture)

# 绘制Prophet预测结果
fig = model.plot(forecast)
plt.title('Prophet Forecast')
plt.show()

# 绘制组件图
fig = model.plot_components(forecast)
plt.show()
```

## 第二章 机器学习在数据分析中的应用

### 2.1 机器学习概述

机器学习是一种让计算机从数据中学习并做出预测或决策的方法，不需要明确编程。在数据分析中，机器学习可以帮助我们发现数据中的复杂模式和关系，进行预测和分类。

**机器学习的主要类型**：
- 监督学习（Supervised Learning）：使用标记数据进行训练
- 无监督学习（Unsupervised Learning）：使用未标记数据进行训练
- 半监督学习（Semi-supervised Learning）：结合标记和未标记数据进行训练
- 强化学习（Reinforcement Learning）：通过与环境交互并接收反馈进行学习

### 2.2 监督学习算法

监督学习是最常用的机器学习类型，适用于分类和回归问题。

#### 2.2.1 分类算法

分类算法用于预测离散的类别标签。

**常见的分类算法**：
- 逻辑回归（Logistic Regression）
- 决策树（Decision Tree）
- 随机森林（Random Forest）
- 支持向量机（SVM, Support Vector Machine）
- 朴素贝叶斯（Naive Bayes）
- K近邻（KNN, K-Nearest Neighbors）
- 梯度提升树（Gradient Boosting Tree）
- 神经网络（Neural Network）

**Python实现**：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

# 加载数据集
data = load_breast_cancer()
X, y = data.data, data.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 数据标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 训练多个分类模型
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(probability=True, random_state=42)
}

# 评估模型性能
results = {}
for name, model in models.items():
    # 训练模型
    model.fit(X_train_scaled, y_train)
    
    # 在测试集上进行预测
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test_scaled)
    
    # 计算评估指标
    report = classification_report(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    
    # 计算ROC曲线和AUC
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    
    results[name] = {
        'report': report,
        'confusion_matrix': cm,
        'roc_auc': roc_auc,
        'fpr': fpr,
        'tpr': tpr
    }
    
    print(f"\n{name} Classification Report:")
    print(report)

# 绘制ROC曲线
plt.figure(figsize=(10, 8))
for name, result in results.items():
    plt.plot(result['fpr'], result['tpr'], lw=2, label=f'{name} (AUC = {result['roc_auc']:.3f})')
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# 选择性能最好的模型进行超参数调优
best_model_name = max(results, key=lambda x: results[x]['roc_auc'])
print(f"\nBest model: {best_model_name}")

# 对最佳模型进行超参数调优
if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10]
    }
    grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='roc_auc')
    grid_search.fit(X_train_scaled, y_train)
    
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best cross-validation score: {grid_search.best_score_:.4f}")
    
    # 使用最佳参数的模型进行最终预测
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test_scaled)
print("\nTuned Model Classification Report:")
print(classification_report(y_test, y_pred_best))
```

#### 2.2.2 回归算法

回归算法用于预测连续的数值。

**常见的回归算法**：
- 线性回归（Linear Regression）
- 岭回归（Ridge Regression）
- Lasso回归（Lasso Regression）
- 决策树回归（Decision Tree Regression）
- 随机森林回归（Random Forest Regression）
- 梯度提升回归（Gradient Boosting Regression）
- 支持向量回归（SVR, Support Vector Regression）
- 神经网络回归（Neural Network Regression）

**Python实现**：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

# 加载数据集
data = fetch_california_housing()
X, y = data.data, data.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 数据标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 训练多个回归模型
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(random_state=42),
    'Lasso Regression': Lasso(random_state=42),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42),
    'SVR': SVR()
}

# 评估模型性能
results = {}
for name, model in models.items():
    # 训练模型
    model.fit(X_train_scaled, y_train)
    
    # 在测试集上进行预测
    y_pred = model.predict(X_test_scaled)
    
    # 计算评估指标
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    results[name] = {
        'mse': mse,
        'rmse': rmse,
        'r2': r2,
        'predictions': y_pred
    }
    
    print(f"{name}: MSE = {mse:.4f}, RMSE = {rmse:.4f}, R2 = {r2:.4f}")

# 绘制预测结果与实际值的对比
plt.figure(figsize=(12, 8))
plt.scatter(y_test, y_test, color='black', label='Perfect Prediction')

for name, result in results.items():
    plt.scatter(y_test, result['predictions'], alpha=0.5, label=name)

plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.legend()
plt.grid(True)
plt.show()

# 选择性能最好的模型进行超参数调优
best_model_name = max(results, key=lambda x: results[x]['r2'])
print(f"\nBest model: {best_model_name}")

# 对最佳模型进行超参数调优
if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10]
    }
    grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='r2')
    grid_search.fit(X_train_scaled, y_train)
    
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best cross-validation score: {grid_search.best_score_:.4f}")
    
    # 使用最佳参数的模型进行最终预测
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test_scaled)
print(f"\nTuned Model: MSE = {mean_squared_error(y_test, y_pred_best):.4f}, RMSE = {np.sqrt(mean_squared_error(y_test, y_pred_best)):.4f}, R2 = {r2_score(y_test, y_pred_best):.4f}")
```

### 2.3 无监督学习算法

无监督学习用于发现数据中的隐藏模式和结构，适用于聚类、降维和关联规则挖掘等任务。

#### 2.3.1 聚类算法

聚类算法用于将相似的数据点分组到同一个簇中。

**常见的聚类算法**：
- K均值聚类（K-Means Clustering）
- 层次聚类（Hierarchical Clustering）
- DBSCAN（Density-Based Spatial Clustering of Applications with Noise）
- 高斯混合模型（GMM, Gaussian Mixture Model）

**Python实现**：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs, load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.decomposition import PCA
import scipy.cluster.hierarchy as sch

# 生成示例数据
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)

# 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 训练多个聚类模型
models = {
    'K-Means': KMeans(n_clusters=4, random_state=42),
    'Hierarchical': AgglomerativeClustering(n_clusters=4),
    'DBSCAN': DBSCAN(eps=0.3, min_samples=5),
    'GMM': GaussianMixture(n_components=4, random_state=42)
}

# 评估模型性能
results = {}
for name, model in models.items():
    # 训练模型
    if name == 'GMM':
        model.fit(X_scaled)
        labels = model.predict(X_scaled)
    else:
        labels = model.fit_predict(X_scaled)
    
    # 计算评估指标（注意：DBSCAN可能产生噪声点，标签为-1）
    if name != 'DBSCAN' or len(set(labels)) > 1:
        # 过滤掉噪声点
        valid_labels = labels[labels != -1] if name == 'DBSCAN' else labels
        valid_X = X_scaled[labels != -1] if name == 'DBSCAN' else X_scaled
        
        if len(set(valid_labels)) > 1:
            silhouette = silhouette_score(valid_X, valid_labels)
            davies_bouldin = davies_bouldin_score(valid_X, valid_labels)
            
            results[name] = {
                'labels': labels,
                'silhouette_score': silhouette,
                'davies_bouldin_score': davies_bouldin
            }
            
            print(f"{name}: Silhouette Score = {silhouette:.4f}, Davies-Bouldin Index = {davies_bouldin:.4f}")

# 绘制聚类结果
plt.figure(figsize=(15, 10))

for i, (name, result) in enumerate(results.items(), 1):
    plt.subplot(2, 2, i)
    # 对于DBSCAN，将噪声点标记为不同的颜色
    if name == 'DBSCAN':
        # 绘制聚类点
        for label in set(result['labels']):
            if label == -1:
                # 噪声点
                plt.scatter(X_scaled[result['labels'] == label, 0], X_scaled[result['labels'] == label, 1], 
                           c='black', marker='x', s=50, label='Noise')
            else:
                # 聚类点
                plt.scatter(X_scaled[result['labels'] == label, 0], X_scaled[result['labels'] == label, 1], 
                           s=50, label=f'Cluster {label}')
    else:
        # 绘制其他聚类算法的结果
        plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=result['labels'], s=50, cmap='viridis')
        
    plt.title(f'{name} Clustering')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    if name == 'DBSCAN':
        plt.legend()

plt.tight_layout()
plt.show()

# 层次聚类的树状图
plt.figure(figsize=(10, 7))
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Samples')
plt.ylabel('Euclidean distances')
plt.show()

# 在真实数据集上应用聚类（鸢尾花数据集）
iris = load_iris()
X_iris, y_iris = iris.data, iris.target
X_iris_scaled = scaler.fit_transform(X_iris)

# 使用PCA降维以便可视化
pca = PCA(n_components=2)
X_iris_pca = pca.fit_transform(X_iris_scaled)

# 应用K-Means聚类
kmeans_iris = KMeans(n_clusters=3, random_state=42)
y_kmeans = kmeans_iris.fit_predict(X_iris_scaled)

# 绘制聚类结果与真实标签的对比
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=y_kmeans, s=50, cmap='viridis')
plt.title('K-Means Clustering Results')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')

plt.subplot(1, 2, 2)
plt.scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=y_iris, s=50, cmap='viridis')
plt.title('True Labels')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')

plt.tight_layout()
plt.show()
```

#### 2.3.2 降维算法

降维算法用于减少数据的维度，同时保留数据的重要信息，适用于数据可视化和特征工程。

**常见的降维算法**：
- 主成分分析（PCA, Principal Component Analysis）
- 线性判别分析（LDA, Linear Discriminant Analysis）
- t-SNE（t-Distributed Stochastic Neighbor Embedding）
- UMAP（Uniform Manifold Approximation and Projection）
- 自动编码器（Autoencoder）

**Python实现**：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA, KernelPCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.manifold import TSNE
import umap

# 加载数据集
digits = load_digits()
X, y = digits.data, digits.target

# 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 应用不同的降维算法
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

lda = LDA(n_components=2)
X_lda = lda.fit_transform(X_scaled, y)

tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X_scaled)

umap_model = umap.UMAP(n_components=2, random_state=42)
X_umap = umap_model.fit_transform(X_scaled)

# 可视化降维结果
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# PCA结果
scatter = axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10')
axes[0, 0].set_title('PCA')
axes[0, 0].set_xlabel('Principal Component 1')
axes[0, 0].set_ylabel('Principal Component 2')
axes[0, 0].legend(handles=scatter.legend_elements()[0], labels=list(range(10)))

# LDA结果
scatter = axes[0, 1].scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='tab10')
axes[0, 1].set_title('LDA')
axes[0, 1].set_xlabel('Linear Discriminant 1')
axes[0, 1].set_ylabel('Linear Discriminant 2')
axes[0, 1].legend(handles=scatter.legend_elements()[0], labels=list(range(10)))

# t-SNE结果
scatter = axes[1, 0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10')
axes[1, 0].set_title('t-SNE')
axes[1, 0].set_xlabel('t-SNE Component 1')
axes[1, 0].set_ylabel('t-SNE Component 2')
axes[1, 0].legend(handles=scatter.legend_elements()[0], labels=list(range(10)))

# UMAP结果
scatter = axes[1, 1].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10')
axes[1, 1].set_title('UMAP')
axes[1, 1].set_xlabel('UMAP Component 1')
axes[1, 1].set_ylabel('UMAP Component 2')
axes[1, 1].legend(handles=scatter.legend_elements()[0], labels=list(range(10)))

plt.tight_layout()
plt.show()

# 评估PCA的方差解释率
pca_full = PCA().fit(X_scaled)
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca_full.explained_variance_ratio_), 'o-')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.title('PCA Cumulative Explained Variance')
plt.grid(True)
plt.show()

# 确定保留95%方差所需的主成分数量
n_components = np.argmax(np.cumsum(pca_full.explained_variance_ratio_) >= 0.95) + 1
print(f'Number of components needed to explain 95% of variance: {n_components}')
```

## 第三章 特征工程与模型优化

### 3.1 特征工程

特征工程是数据分析和机器学习中最重要的环节之一，它涉及创建、选择和转换特征，以提高模型的性能。

#### 3.1.1 特征创建

特征创建是从原始数据中创建新的特征。

**常见的特征创建方法**：
- 数学运算（加、减、乘、除等）
- 多项式特征
- 交互特征
- 分组统计特征
- 时间特征（如年月日、季节、节假日等）
- 文本特征（如词频、TF-IDF等）

**Python实现**：

```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import PolynomialFeatures

# 创建示例数据
df = pd.DataFrame({
    'age': [25, 30, 35, 40, 45],
    'income': [50000, 60000, 75000, 90000, 100000],
    'debt': [10000, 15000, 20000, 18000, 25000],
    'purchase_date': pd.to_datetime(['2023-01-15', '2023-02-20', '2023-03-10', '2023-04-05', '2023-05-12'])
})

# 数学运算创建特征
df['debt_to_income'] = df['debt'] / df['income']
df['income_per_age'] = df['income'] / df['age']

# 多项式特征
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(df[['age', 'income']])
poly_feature_names = poly.get_feature_names_out(['age', 'income'])
df_poly = pd.DataFrame(poly_features, columns=poly_feature_names)
df = pd.concat([df, df_poly], axis=1)

# 时间特征提取
df['year'] = df['purchase_date'].dt.year
df['month'] = df['purchase_date'].dt.month
df['day'] = df['purchase_date'].dt.day
df['day_of_week'] = df['purchase_date'].dt.dayofweek
df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

print("创建的特征:")
print(df.head())
```

#### 3.1.2 特征选择

特征选择是从所有特征中选择最相关的特征子集，以减少模型的复杂性和过拟合风险。

**常见的特征选择方法**：
- 过滤法（Filter Methods）：如相关系数、卡方检验、互信息等
- 包装法（Wrapper Methods）：如递归特征消除（RFE）、序列特征选择等
- 嵌入法（Embedded Methods）：如Lasso回归、随机森林特征重要性等

**Python实现**：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif, RFE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# 加载数据集
data = load_breast_cancer()
X, y = data.data, data.target
feature_names = data.feature_names

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 数据标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 过滤法：使用SelectKBest选择最佳特征
# 使用卡方检验
selector_chi2 = SelectKBest(chi2, k=10)
X_train_chi2 = selector_chi2.fit_transform(X_train_scaled, y_train)
selected_features_chi2 = feature_names[selector_chi2.get_support()]
print("\n卡方检验选择的特征:")
print(selected_features_chi2)

# 使用F检验
selector_f = SelectKBest(f_classif, k=10)
X_train_f = selector_f.fit_transform(X_train_scaled, y_train)
selected_features_f = feature_names[selector_f.get_support()]
print("\nF检验选择的特征:")
print(selected_features_f)

# 使用互信息
selector_mi = SelectKBest(mutual_info_classif, k=10)
X_train_mi = selector_mi.fit_transform(X_train_scaled, y_train)
selected_features_mi = feature_names[selector_mi.get_support()]
print("\n互信息选择的特征:")
print(selected_features_mi)

# 包装法：使用RFE选择特征
estimator = LogisticRegression(random_state=42)
rfe = RFE(estimator, n_features_to_select=10)
X_train_rfe = rfe.fit_transform(X_train_scaled, y_train)
selected_features_rfe = feature_names[rfe.support_]
print("\nRFE选择的特征:")
print(selected_features_rfe)

# 嵌入法：使用随机森林的特征重要性
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_scaled, y_train)

# 获取特征重要性
feature_importance = rf.feature_importances_
feature_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print("\n随机森林特征重要性前10名:")
print(feature_importance_df.head(10))

# 可视化特征重要性
plt.figure(figsize=(12, 8))
plt.barh(feature_importance_df['feature'][:10][::-1], feature_importance_df['importance'][:10][::-1])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Top 10 Features Importance (Random Forest)')
plt.show()
```

#### 3.1.3 特征转换

特征转换是将原始特征转换为更适合模型使用的形式。

**常见的特征转换方法**：
- 标准化（Standardization）
- 归一化（Normalization）
- 对数变换（Log Transformation）
- 幂变换（Power Transformation）
- 离散化（Discretization）
- 独热编码（One-Hot Encoding）
- 标签编码（Label Encoding）

**Python实现**：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, KBinsDiscretizer, OneHotEncoder, LabelEncoder
from scipy import stats

# 创建示例数据
df = pd.DataFrame({
    'age': [25, 30, 35, 40, 45, 50, 55, 60],
    'income': [30000, 45000, 60000, 75000, 90000, 105000, 120000, 135000],
    'debt': [5000, 10000, 15000, 20000, 25000, 30000, 35000, 40000],
    'education': ['本科', '硕士', '本科', '博士', '硕士', '本科', '博士', '硕士'],
    'occupation': ['工程师', '分析师', '工程师', '科学家', '分析师', '工程师', '科学家', '分析师']
})

# 数值特征转换
# 标准化
scaler_std = StandardScaler()
age_std = scaler_std.fit_transform(df[['age']])

# 归一化
scaler_minmax = MinMaxScaler()
income_minmax = scaler_minmax.fit_transform(df[['income']])

# RobustScaler（对异常值不敏感）
scaler_robust = RobustScaler()
debt_robust = scaler_robust.fit_transform(df[['debt']])

# 幂变换（处理偏态数据）
pt = PowerTransformer(method='yeo-johnson')
income_transformed = pt.fit_transform(df[['income']])

# 离散化
kbd = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')
age_binned = kbd.fit_transform(df[['age']])

# 分类特征编码
# 标签编码
le = LabelEncoder()
education_encoded = le.fit_transform(df['education'])

# 独热编码
encoder = OneHotEncoder(sparse_output=False)
occupation_encoded = encoder.fit_transform(df[['occupation']])
occupation_encoded_df = pd.DataFrame(occupation_encoded, columns=encoder.get_feature_names_out(['occupation']))

# 可视化变换效果
plt.figure(figsize=(15, 10))

# 原始收入分布
plt.subplot(2, 3, 1)
plt.hist(df['income'], bins=5)
plt.title('Original Income Distribution')
plt.xlabel('Income')
plt.ylabel('Frequency')

# 标准化后的年龄分布
plt.subplot(2, 3, 2)
plt.hist(age_std, bins=5)
plt.title('Standardized Age Distribution')
plt.xlabel('Standardized Age')
plt.ylabel('Frequency')

# 幂变换后的收入分布
plt.subplot(2, 3, 3)
plt.hist(income_transformed, bins=5)
plt.title('Power Transformed Income Distribution')
plt.xlabel('Transformed Income')
plt.ylabel('Frequency')

# 离散化后的年龄分布
plt.subplot(2, 3, 4)
plt.hist(age_binned, bins=3)
plt.title('Binned Age Distribution')
plt.xlabel('Age Bin')
plt.ylabel('Frequency')

# 教育程度标签编码
plt.subplot(2, 3, 5)
plt.bar(le.classes_, range(len(le.classes_)))
plt.title('Education Label Encoding')
plt.xlabel('Education')
plt.ylabel('Encoded Value')

# 职业独热编码
plt.subplot(2, 3, 6)
plt.bar(occupation_encoded_df.columns, occupation_encoded_df.sum(axis=0))
plt.title('Occupation One-Hot Encoding')
plt.xlabel('Occupation')
plt.ylabel('Count')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# 查看转换后的数据
transformed_df = pd.DataFrame({
    'age_std': age_std.flatten(),
    'income_minmax': income_minmax.flatten(),
    'debt_robust': debt_robust.flatten(),
    'income_transformed': income_transformed.flatten(),
    'age_binned': age_binned.flatten(),
    'education_encoded': education_encoded
})

transformed_df = pd.concat([transformed_df, occupation_encoded_df], axis=1)
print("转换后的数据:")
print(transformed_df.head())
```

### 3.2 模型优化

模型优化是提高机器学习模型性能的关键步骤，包括超参数调优、模型集成和避免过拟合等技术。

#### 3.2.1 超参数调优

超参数调优是寻找模型最佳超参数组合的过程。

**常见的超参数调优方法**：
- 网格搜索（Grid Search）
- 随机搜索（Random Search）
- 贝叶斯优化（Bayesian Optimization）
- 梯度-based优化（Gradient-based Optimization）
- 进化算法（Evolutionary Algorithms）

**Python实现**：

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from scipy.stats import randint as sp_randint
from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer

# 加载数据集
data = load_breast_cancer()
X, y = data.data, data.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 数据标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 定义基础模型
base_model = RandomForestClassifier(random_state=42)

# 方法1：网格搜索
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

grid_search = GridSearchCV(estimator=base_model,
                           param_grid=param_grid,
                           cv=5,
                           scoring='roc_auc',
                           n_jobs=-1,
                           verbose=1)

grid_search.fit(X_train_scaled, y_train)

print("\n网格搜索结果:")
print(f"最佳参数: {grid_search.best_params_}")
print(f"最佳交叉验证分数: {grid_search.best_score_:.4f}")

# 使用最佳参数的模型进行预测
grid_best_model = grid_search.best_estimator_
y_pred_grid = grid_best_model.predict(X_test_scaled)
print("\n网格搜索最佳模型的分类报告:")
print(classification_report(y_test, y_pred_grid))

# 方法2：随机搜索
param_dist = {
    'n_estimators': sp_randint(50, 200),
    'max_depth': [None] + list(sp_randint(10, 30).rvs(3)),
    'min_samples_split': sp_randint(2, 10),
    'min_samples_leaf': sp_randint(1, 5),
    'max_features': ['sqrt', 'log2', None]
}

random_search = RandomizedSearchCV(estimator=base_model,
                                  param_distributions=param_dist,
                                  n_iter=50,
                                  cv=5,
                                  scoring='roc_auc',
                                  n_jobs=-1,
                                  random_state=42,
                                  verbose=1)

random_search.fit(X_train_scaled, y_train)

print("\n随机搜索结果:")
print(f"最佳参数: {random_search.best_params_}")
print(f"最佳交叉验证分数: {random_search.best_score_:.4f}")

# 使用最佳参数的模型进行预测
random_best_model = random_search.best_estimator_
y_pred_random = random_best_model.predict(X_test_scaled)
print("\n随机搜索最佳模型的分类报告:")
print(classification_report(y_test, y_pred_random))

# 方法3：贝叶斯优化（需要安装scikit-optimize）
param_space = {
    'n_estimators': Integer(50, 200),
    'max_depth': Integer(10, 30),
    'min_samples_split': Integer(2, 10),
    'min_samples_leaf': Integer(1, 5),
    'max_features': Categorical(['sqrt', 'log2', None])
}

bayes_search = BayesSearchCV(estimator=base_model,
                            search_spaces=param_space,
                            n_iter=50,
                            cv=5,
                            scoring='roc_auc',
                            n_jobs=-1,
                            random_state=42,
                            verbose=1)

bayes_search.fit(X_train_scaled, y_train)

print("\n贝叶斯优化结果:")
print(f"最佳参数: {bayes_search.best_params_}")
print(f"最佳交叉验证分数: {bayes_search.best_score_:.4f}")

# 使用最佳参数的模型进行预测
bayes_best_model = bayes_search.best_estimator_
y_pred_bayes = bayes_best_model.predict(X_test_scaled)
print("\n贝叶斯优化最佳模型的分类报告:")
print(classification_report(y_test, y_pred_bayes))
```

#### 3.2.2 模型集成

模型集成是结合多个模型的预测结果，以提高整体预测性能的技术。

**常见的模型集成方法**：
- 投票法（Voting）
- 堆叠法（Stacking）
- 袋装法（Bagging）
- 提升法（Boosting）

**Python实现**：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,
                             AdaBoostClassifier, VotingClassifier, BaggingClassifier)
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, roc_curve, auc
from mlxtend.classifier import StackingCVClassifier  # 需要安装mlxtend

# 加载数据集
data = load_breast_cancer()
X, y = data.data, data.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 数据标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 定义基础模型
base_models = [
    ('lr', LogisticRegression(random_state=42)),
    ('dt', DecisionTreeClassifier(random_state=42)),
    ('rf', RandomForestClassifier(random_state=42)),
    ('gb', GradientBoostingClassifier(random_state=42)),
    ('svc', SVC(probability=True, random_state=42)),
    ('knn', KNeighborsClassifier())
]

# 1. 投票集成
voting_clf = VotingClassifier(estimators=base_models,
                             voting='soft',  # 使用软投票（概率加权）
                             n_jobs=-1)

voting_clf.fit(X_train_scaled, y_train)
y_pred_voting = voting_clf.predict(X_test_scaled)
y_pred_proba_voting = voting_clf.predict_proba(X_test_scaled)[:, 1]

print("\n投票集成模型分类报告:")
print(classification_report(y_test, y_pred_voting))

# 2. 堆叠集成
# 定义元分类器
meta_classifier = LogisticRegression(random_state=42)

# 创建堆叠分类器
stacking_clf = StackingCVClassifier(classifiers=[model for name, model in base_models],
                                   meta_classifier=meta_classifier,
                                   cv=5,
                                   use_probas=True,
                                   n_jobs=-1,
                                   random_state=42)

stacking_clf.fit(X_train_scaled, y_train)
y_pred_stacking = stacking_clf.predict(X_test_scaled)

try:
    y_pred_proba_stacking = stacking_clf.predict_proba(X_test_scaled)[:, 1]
except AttributeError:
    # 某些情况下，StackingCVClassifier可能没有predict_proba方法
    y_pred_proba_stacking = None

print("\n堆叠集成模型分类报告:")
print(classification_report(y_test, y_pred_stacking))

# 3. Bagging集成
# 使用决策树作为基础模型的Bagging
bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42),
                               n_estimators=100,
                               max_samples=0.8,
                               max_features=0.8,
                               bootstrap=True,
                               bootstrap_features=False,
                               n_jobs=-1,
                               random_state=42)

bagging_clf.fit(X_train_scaled, y_train)
y_pred_bagging = bagging_clf.predict(X_test_scaled)
y_pred_proba_bagging = bagging_clf.predict_proba(X_test_scaled)[:, 1]

print("\nBagging集成模型分类报告:")
print(classification_report(y_test, y_pred_bagging))

# 4. AdaBoost集成
adaboost_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1, random_state=42),
                                n_estimators=100,
                                learning_rate=0.1,
                                random_state=42)

adaboost_clf.fit(X_train_scaled, y_train)
y_pred_adaboost = adaboost_clf.predict(X_test_scaled)
y_pred_proba_adaboost = adaboost_clf.predict_proba(X_test_scaled)[:, 1]

print("\nAdaBoost集成模型分类报告:")
print(classification_report(y_test, y_pred_adaboost))

# 评估所有集成模型的性能
models = {
    'Voting Ensemble': (y_pred_proba_voting, y_pred_voting),
    'Stacking Ensemble': (y_pred_proba_stacking, y_pred_stacking),
    'Bagging Ensemble': (y_pred_proba_bagging, y_pred_bagging),
    'AdaBoost Ensemble': (y_pred_proba_adaboost, y_pred_adaboost)
}

# 绘制ROC曲线
plt.figure(figsize=(10, 8))

for name, (proba, pred) in models.items():
    if proba is not None:
        fpr, tpr, _ = roc_curve(y_test, proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Ensemble Models')
plt.legend(loc="lower right")
plt.show()

# 比较基础模型和集成模型的性能
# 计算基础模型的ROC AUC
base_auc_scores = {}
for name, model in base_models:
    model.fit(X_train_scaled, y_train)
    if hasattr(model, 'predict_proba'):
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        base_auc_scores[name] = roc_auc

# 计算集成模型的ROC AUC
ensemble_auc_scores = {}
for name, (proba, pred) in models.items():
    if proba is not None:
        fpr, tpr, _ = roc_curve(y_test, proba)
        roc_auc = auc(fpr, tpr)
        ensemble_auc_scores[name] = roc_auc

# 合并结果进行比较
base_auc_df = pd.DataFrame(list(base_auc_scores.items()), columns=['Model', 'ROC AUC'])
ensemble_auc_df = pd.DataFrame(list(ensemble_auc_scores.items()), columns=['Model', 'ROC AUC'])

print("\n基础模型ROC AUC分数:")
print(base_auc_df.sort_values('ROC AUC', ascending=False))

print("\n集成模型ROC AUC分数:")
print(ensemble_auc_df.sort_values('ROC AUC', ascending=False))
```

#### 3.2.3 避免过拟合

过拟合是机器学习中的常见问题，指模型在训练数据上表现良好，但在新数据上表现不佳。以下是一些避免过拟合的方法：

**常见的避免过拟合的方法**：
- 增加训练数据量
- 简化模型复杂度
- 正则化（Regularization）
- 交叉验证（Cross-Validation）
- 早停法（Early Stopping）
- Dropout（神经网络）
- 数据增强（Data Augmentation）

**Python实现**：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.exceptions import ConvergenceWarning
import warnings

# 忽略警告
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# 加载数据集
data = fetch_california_housing()
X, y = data.data, data.target

# 划分训练集和验证集和测试集
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)

# 数据标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# 1. 简化模型复杂度
# 训练不同深度的决策树，观察过拟合情况
depths = range(1, 31)
train_scores = []
val_scores = []

for depth in depths:
    tree = DecisionTreeRegressor(max_depth=depth, random_state=42)
    tree.fit(X_train_scaled, y_train)
    
    # 在训练集上的得分
    y_train_pred = tree.predict(X_train_scaled)
    train_score = r2_score(y_train, y_train_pred)
    train_scores.append(train_score)
    
    # 在验证集上的得分
    y_val_pred = tree.predict(X_val_scaled)
    val_score = r2_score(y_val, y_val_pred)
    val_scores.append(val_score)

# 找到最佳深度
best_depth = depths[np.argmax(val_scores)]
print(f"最佳决策树深度: {best_depth}")

# 绘制不同深度的训练和验证得分
plt.figure(figsize=(10, 6))
plt.plot(depths, train_scores, marker='o', label='Training R2 Score')
plt.plot(depths, val_scores, marker='s', label='Validation R2 Score')
plt.axvline(x=best_depth, color='r', linestyle='--', label=f'Best Depth = {best_depth}')
plt.xlabel('Tree Depth')
plt.ylabel('R2 Score')
plt.title('Decision Tree Depth vs Performance')
plt.legend()
plt.grid(True)
plt.show()

# 2. 正则化
# 线性回归（无正则化）
linear_reg = LinearRegression()
linear_reg.fit(X_train_scaled, y_train)
y_train_pred_linear = linear_reg.predict(X_train_scaled)
y_test_pred_linear = linear_reg.predict(X_test_scaled)

# Ridge回归（L2正则化）
ridge = Ridge(alpha=1.0, random_state=42)
ridge.fit(X_train_scaled, y_train)
y_train_pred_ridge = ridge.predict(X_train_scaled)
y_test_pred_ridge = ridge.predict(X_test_scaled)

# Lasso回归（L1正则化）
lasso = Lasso(alpha=0.1, random_state=42)
lasso.fit(X_train_scaled, y_train)
y_train_pred_lasso = lasso.predict(X_train_scaled)
y_test_pred_lasso = lasso.predict(X_test_scaled)

# 评估模型性能
models = {
    'Linear Regression': (y_train_pred_linear, y_test_pred_linear),
    'Ridge Regression': (y_train_pred_ridge, y_test_pred_ridge),
    'Lasso Regression': (y_train_pred_lasso, y_test_pred_lasso)
}

for name, (y_train_pred, y_test_pred) in models.items():
    train_mse = mean_squared_error(y_train, y_train_pred)
    train_rmse = np.sqrt(train_mse)
    train_r2 = r2_score(y_train, y_train_pred)
    
    test_mse = mean_squared_error(y_test, y_test_pred)
    test_rmse = np.sqrt(test_mse)
    test_r2 = r2_score(y_test, y_test_pred)
    
    print(f"\n{name}:")
    print(f"  训练集: RMSE = {train_rmse:.4f}, R2 = {train_r2:.4f}")
    print(f"  测试集: RMSE = {test_rmse:.4f}, R2 = {test_r2:.4f}")
    print(f"  过拟合程度: {abs(train_r2 - test_r2):.4f}")

# 3. 使用GridSearchCV进行超参数调优和交叉验证
best_tree = DecisionTreeRegressor(max_depth=best_depth, random_state=42)

# 随机森林（自带集成和正则化效果）
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'max_features': ['sqrt', 'log2']
}

rf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42),
                             param_grid=rf_param_grid,
                             cv=5,
                             scoring='r2',
                             n_jobs=-1,
                             verbose=1)

rf_grid_search.fit(X_train_scaled, y_train)

print(f"\n随机森林最佳参数: {rf_grid_search.best_params_}")
print(f"随机森林最佳交叉验证分数: {rf_grid_search.best_score_:.4f}")

# 在测试集上评估最佳随机森林模型
best_rf = rf_grid_search.best_estimator_
y_test_pred_rf = best_rf.predict(X_test_scaled)
rf_test_mse = mean_squared_error(y_test, y_test_pred_rf)
rf_test_rmse = np.sqrt(rf_test_mse)
rf_test_r2 = r2_score(y_test, y_test_pred_rf)

print(f"\n最佳随机森林模型在测试集上的性能:")
print(f"  RMSE = {rf_test_rmse:.4f}, R2 = {rf_test_r2:.4f}")

# 比较所有模型在测试集上的性能
# 先完成决策树模型的评估
best_tree.fit(X_train_scaled, y_train)
y_test_pred_tree = best_tree.predict(X_test_scaled)
tree_test_r2 = r2_score(y_test, y_test_pred_tree)

results = {
    'Linear Regression': test_r2,
    'Ridge Regression': r2_score(y_test, y_test_pred_ridge),
    'Lasso Regression': r2_score(y_test, y_test_pred_lasso),
    'Decision Tree (Best Depth)': tree_test_r2,
    'Random Forest (Tuned)': rf_test_r2
}

# 排序结果
sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)

print("\n所有模型在测试集上的R2得分（从高到低）:")
for name, score in sorted_results:
    print(f"  {name}: {score:.4f}")

# 绘制模型性能比较图
plt.figure(figsize=(12, 6))
names = [name for name, score in sorted_results]
scores = [score for name, score in sorted_results]
plt.barh(names, scores, color='skyblue')
plt.xlabel('R2 Score')
plt.ylabel('Model')
plt.title('Model Performance Comparison on Test Data')
plt.grid(True, axis='x')
plt.xlim(0, 1)

# 在条形图上添加数值标签
for i, v in enumerate(scores):
    plt.text(v + 0.01, i, f'{v:.4f}', va='center')

plt.show()

# 4. 早停法示例（在神经网络中更常用，但这里用梯度提升树演示）
from sklearn.ensemble import GradientBoostingRegressor
import numpy as np

# 记录训练和验证的误差
train_errors = []
val_errors = []
models = []

# 逐步增加树的数量，观察验证误差的变化
for n_estimators in range(1, 201):
    gb = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=3, random_state=42)
    gb.fit(X_train_scaled, y_train)
    
    # 计算训练误差
    y_train_pred = gb.predict(X_train_scaled)
    train_error = mean_squared_error(y_train, y_train_pred)
    train_errors.append(train_error)
    
    # 计算验证误差
    y_val_pred = gb.predict(X_val_scaled)
    val_error = mean_squared_error(y_val, y_val_pred)
    val_errors.append(val_error)
    
    # 保存模型
    models.append(gb)

# 找到验证误差最小的点
best_n_estimators = np.argmin(val_errors) + 1
best_val_error = val_errors[best_n_estimators - 1]
best_model = models[best_n_estimators - 1]

print(f"\n早停法找到的最佳树数量: {best_n_estimators}")
print(f"对应的验证集误差: {best_val_error:.4f}")

# 在测试集上评估最佳模型
y_test_pred_early_stop = best_model.predict(X_test_scaled)
early_stop_test_error = mean_squared_error(y_test, y_test_pred_early_stop)
early_stop_test_r2 = r2_score(y_test, y_test_pred_early_stop)

print(f"早停法最佳模型在测试集上的性能:")
print(f"  测试集误差: {early_stop_test_error:.4f}")
print(f"  测试集R2得分: {early_stop_test_r2:.4f}")

# 绘制早停法的学习曲线
plt.figure(figsize=(10, 6))
plt.plot(range(1, 201), train_errors, marker='o', label='Training Error')
plt.plot(range(1, 201), val_errors, marker='s', label='Validation Error')
plt.axvline(x=best_n_estimators, color='r', linestyle='--', label=f'Best n_estimators = {best_n_estimators}')
plt.xlabel('Number of Trees')
plt.ylabel('Mean Squared Error')
plt.title('Early Stopping Learning Curve')
plt.legend()
plt.grid(True)
plt.show()

# 5. 数据增强示例（主要用于图像和文本数据，但这里用简单的数值数据演示）
from sklearn.utils import resample

# 原始数据量
print(f"原始训练集大小: {X_train_scaled.shape[0]}")

# 数据增强 - 随机采样（有放回）
X_train_augmented, y_train_augmented = resample(X_train_scaled, y_train,
                                               replace=True,
                                               n_samples=int(X_train_scaled.shape[0] * 1.5),
                                               random_state=42)

print(f"增强后训练集大小: {X_train_augmented.shape[0]}")

# 在增强后的数据集上训练模型
best_rf_augmented = RandomForestRegressor(**rf_grid_search.best_params_, random_state=42)
best_rf_augmented.fit(X_train_augmented, y_train_augmented)

y_test_pred_rf_augmented = best_rf_augmented.predict(X_test_scaled)
rf_augmented_test_r2 = r2_score(y_test, y_test_pred_rf_augmented)

print(f"\n在增强数据集上训练的随机森林模型在测试集上的R2得分: {rf_augmented_test_r2:.4f}")
print(f"原始数据集训练的模型R2得分: {rf_test_r2:.4f}")
print(f"性能提升: {(rf_augmented_test_r2 - rf_test_r2):.4f}")

# 总结避免过拟合的方法
print("\n避免过拟合的主要方法总结:")
print("1. 增加训练数据量或进行数据增强")
print("2. 简化模型复杂度（如限制决策树深度）")
print("3. 使用正则化技术（L1、L2正则化）")
print("4. 采用交叉验证进行模型评估和选择")
print("5. 使用早停法避免过度训练")
print("6. 采用集成学习方法（如随机森林、梯度提升树）")
print("7. 在神经网络中使用Dropout、Batch Normalization等技术")

## 第四章 实战案例分析

### 4.1 案例一：电商用户行为分析与推荐系统

#### 4.1.1 背景与数据介绍

某电商平台希望通过分析用户的浏览、收藏、购物车和购买行为，构建个性化推荐系统，提高用户转化率和客单价。

**数据概览**：
- 用户数据：用户ID、年龄、性别、注册时间、地理位置等
- 商品数据：商品ID、类别、品牌、价格、描述等
- 行为数据：用户ID、商品ID、行为类型（浏览、收藏、加购物车、购买）、行为时间等

#### 4.1.2 分析目标

1. 分析用户行为模式和偏好
2. 识别高价值用户和潜在流失用户
3. 构建商品推荐模型
4. 评估推荐效果并优化

#### 4.1.3 数据预处理与特征工程

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta

# 假设我们有以下数据
df_user = pd.read_csv('user_data.csv')
df_product = pd.read_csv('product_data.csv')
df_behavior = pd.read_csv('user_behavior_data.csv')

# 数据预处理
# 1. 处理缺失值
df_user = df_user.dropna(subset=['age', 'gender'])
df_behavior = df_behavior.dropna()

# 2. 转换时间格式
df_behavior['behavior_time'] = pd.to_datetime(df_behavior['behavior_time'])

# 3. 数据合并
df = pd.merge(df_behavior, df_user, on='user_id', how='left')
df = pd.merge(df, df_product, on='product_id', how='left')

# 特征工程
# 1. 时间特征提取
df['hour'] = df['behavior_time'].dt.hour
df['day_of_week'] = df['behavior_time'].dt.dayofweek
df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

# 2. 用户行为特征
# 计算每个用户的行为频次
user_behavior_counts = df.groupby(['user_id', 'behavior_type']).size().unstack(fill_value=0)
user_behavior_counts.columns = [f'count_{col}' for col in user_behavior_counts.columns]
user_behavior_counts['total_behavior'] = user_behavior_counts.sum(axis=1)
user_behavior_counts['conversion_rate'] = user_behavior_counts.get('count_purchase', 0) / user_behavior_counts['total_behavior']

# 3. 用户价值特征
# 最近一次购买时间
last_purchase = df[df['behavior_type'] == 'purchase'].groupby('user_id')['behavior_time'].max()
# 计算用户生命周期价值（简化版）
user_value = df[df['behavior_type'] == 'purchase'].groupby('user_id').agg({
    'price': ['sum', 'mean', 'count']
}).reset_index()
user_value.columns = ['user_id', 'total_spent', 'avg_order_value', 'purchase_count']

# 合并所有用户特征
df_user_features = pd.merge(user_behavior_counts.reset_index(), user_value, on='user_id', how='left')
df_user_features = pd.merge(df_user_features, df_user, on='user_id', how='left')

# 填充缺失值
df_user_features = df_user_features.fillna(0)

print("用户特征数据前5行:")
print(df_user_features.head())
```

#### 4.1.4 模型构建与评估

```python
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.cluster import KMeans

# 1. 用户聚类 - 识别高价值用户
# 选择聚类特征
cluster_features = ['total_behavior', 'count_purchase', 'total_spent', 'avg_order_value', 'purchase_count']

# 数据标准化
scaler = StandardScaler()
X_cluster = scaler.fit_transform(df_user_features[cluster_features])

# 使用K-Means进行聚类
kmeans = KMeans(n_clusters=4, random_state=42)
df_user_features['user_segment'] = kmeans.fit_predict(X_cluster)

# 分析每个用户段的特征
segment_analysis = df_user_features.groupby('user_segment')[cluster_features].mean()
print("\n用户段特征分析:")
print(segment_analysis)

# 2. 构建推荐系统
# 这里使用基于协同过滤的推荐算法（使用surprise库）
from surprise import Dataset, Reader, KNNBasic, SVD
from surprise.model_selection import cross_validate

# 准备数据
reader = Reader(rating_scale=(0, 1))
# 我们将购买行为视为1，其他行为视为0.1（简化处理）
rec_data = df.copy()
rec_data['rating'] = rec_data['behavior_type'].apply(lambda x: 1 if x == 'purchase' else 0.1)
rec_data = rec_data[['user_id', 'product_id', 'rating']]

# 转换为surprise数据集
data = Dataset.load_from_df(rec_data, reader)

build_full_trainset = data.build_full_trainset()

# 训练SVD模型
model = SVD()
model.fit(build_full_trainset)

# 评估模型
cv_results = cross_validate(model, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
print(f"\n平均RMSE: {np.mean(cv_results['test_rmse'])}")
print(f"平均MAE: {np.mean(cv_results['test_mae'])}")

# 为指定用户生成推荐
def get_recommendations(user_id, model, trainset, n=10):
    # 获取用户未交互过的商品
    user_items = set([item for (user, item) in trainset.ur[trainset.to_inner_uid(user_id)]])
    all_items = set(trainset.all_items())
    items_to_predict = all_items - user_items
    
    # 为每个商品生成预测评分
    predictions = []
    for item_id in items_to_predict:
        inner_item_id = trainset.to_inner_iid(item_id)
        pred = model.predict(trainset.to_raw_uid(user_id), trainset.to_raw_iid(inner_item_id))
        predictions.append((item_id, pred.est))
    
    # 按预测评分排序并返回前n个
    predictions.sort(key=lambda x: x[1], reverse=True)
    return predictions[:n]

# 为用户1生成10个推荐商品
sample_user_id = df_user_features['user_id'].iloc[0]
recommendations = get_recommendations(sample_user_id, model, build_full_trainset, n=10)

print(f"\n为用户 {sample_user_id} 推荐的商品:")
for product_id, score in recommendations:
    product_info = df_product[df_product['product_id'] == product_id].iloc[0]
    print(f"  商品ID: {product_id}, 类别: {product_info['category']}, 品牌: {product_info['brand']}, 评分: {score:.4f}")

# 3. 预测用户购买行为
# 准备特征和标签
X = df_user_features.drop(['user_id', 'user_segment'], axis=1)
# 将用户分为高价值用户（聚类结果中最高价值的段）和其他用户
top_segment = segment_analysis['total_spent'].idxmax()
y = (df_user_features['user_segment'] == top_segment).astype(int)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建预处理和建模管道
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['age', 'total_behavior', 'count_purchase', 'total_spent', 'avg_order_value', 'purchase_count']),
        ('cat', OneHotEncoder(), ['gender'])
    ]
)

pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# 超参数调优
param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [None, 10, 20],
    'classifier__min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

print(f"\n最佳参数: {grid_search.best_params_}")
print(f"最佳交叉验证AUC分数: {grid_search.best_score_:.4f}")

# 在测试集上评估模型
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

print("\n模型评估结果:")
print(f"测试集AUC分数: {roc_auc_score(y_test, y_pred_proba):.4f}")
print("分类报告:")
print(classification_report(y_test, y_pred))

# 特征重要性分析
# 提取特征重要性
feature_importance = best_model.named_steps['classifier'].feature_importances_

# 获取特征名称
cat_features = best_model.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(['gender'])
num_features = ['age', 'total_behavior', 'count_purchase', 'total_spent', 'avg_order_value', 'purchase_count']
feature_names = np.concatenate([num_features, cat_features])

# 创建特征重要性DataFrame
feature_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print("\n特征重要性前10名:")
print(feature_importance_df.head(10))

# 可视化特征重要性
plt.figure(figsize=(12, 8))
plt.barh(feature_importance_df['feature'][:10][::-1], feature_importance_df['importance'][:10][::-1])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Top 10 Features Importance')
plt.show()
```

#### 4.1.5 业务洞察与建议

基于以上分析，我们得出以下业务洞察和建议：

1. **用户分层管理**：根据用户价值将用户分为不同的 segments，针对高价值用户提供专属优惠和个性化服务，提高用户忠诚度。

2. **精准营销**：根据用户的行为模式和偏好，进行精准营销推送，提高转化率。例如，对于浏览频次高但购买率低的用户，可以发送优惠券或促销信息。

3. **库存优化**：根据推荐系统的结果，优化库存管理，确保热门商品的供应充足。

4. **产品策略**：分析高价值用户的偏好，指导产品开发和采购策略，引入更多符合高价值用户需求的商品。

5. **用户体验优化**：根据用户行为分析，优化网站或APP的用户体验，例如优化商品分类、搜索功能和推荐模块的展示方式。

### 4.2 案例二：金融风险预测与信用评分

#### 4.2.1 背景与数据介绍

某金融机构希望通过分析客户的个人信息、财务状况和历史行为数据，构建信用评分模型，评估客户的信用风险，辅助贷款审批和风险管理决策。

**数据概览**：
- 个人信息：年龄、性别、婚姻状况、教育程度、职业等
- 财务状况：收入、负债、资产、信用历史等
- 行为数据：还款记录、逾期情况、申请记录等
- 标签：是否违约（二分类问题）

#### 4.2.2 分析目标

1. 识别影响信用风险的关键因素
2. 构建信用评分模型，预测客户违约概率
3. 设计信用评分体系，辅助贷款审批决策
4. 评估模型性能并优化

#### 4.2.3 数据预处理与特征工程

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, KBinsDiscretizer
from sklearn.impute import SimpleImputer

# 假设我们有以下数据
df_credit = pd.read_csv('credit_data.csv')

# 数据概览
print("数据集形状:", df_credit.shape)
print("\n数据集前5行:")
print(df_credit.head())

# 检查缺失值
print("\n各列缺失值统计:")
print(df_credit.isnull().sum())

# 数据预处理
# 1. 处理缺失值
# 数值型特征使用中位数填充
num_features = ['age', 'income', 'debt', 'credit_score', 'loan_amount', 'loan_term']
imputer_num = SimpleImputer(strategy='median')
df_credit[num_features] = imputer_num.fit_transform(df_credit[num_features])

# 分类型特征使用众数填充
cat_features = ['gender', 'marital_status', 'education', 'occupation', 'housing_status']
imputer_cat = SimpleImputer(strategy='most_frequent')
df_credit[cat_features] = imputer_cat.fit_transform(df_credit[cat_features])

# 2. 处理异常值
# 使用IQR方法检测和处理异常值
for feature in num_features:
    Q1 = df_credit[feature].quantile(0.25)
    Q3 = df_credit[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # 将异常值替换为上下边界
    df_credit[feature] = df_credit[feature].clip(lower=lower_bound, upper=upper_bound)

# 3. 编码分类特征
# 标签编码
le = LabelEncoder()
for feature in cat_features:
    df_credit[feature] = le.fit_transform(df_credit[feature])

# 特征工程
# 1. 创建衍生特征
# 债务收入比
df_credit['debt_to_income_ratio'] = df_credit['debt'] / df_credit['income']
# 贷款收入比
df_credit['loan_to_income_ratio'] = df_credit['loan_amount'] / df_credit['income']
# 月还款额（简化计算）
df_credit['monthly_payment'] = df_credit['loan_amount'] / (df_credit['loan_term'] * 12)
# 月还款收入比
df_credit['payment_to_income_ratio'] = df_credit['monthly_payment'] / (df_credit['income'] / 12)

# 2. 特征离散化
# 对年龄进行离散化
discretizer_age = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
df_credit['age_bin'] = discretizer_age.fit_transform(df_credit[['age']])

# 对收入进行离散化
discretizer_income = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
df_credit['income_bin'] = discretizer_income.fit_transform(df_credit[['income']])

# 3. 特征交互
# 年龄与收入的交互
df_credit['age_income_interaction'] = df_credit['age'] * df_credit['income']
# 信用评分与债务收入比的交互
df_credit['credit_debt_interaction'] = df_credit['credit_score'] * df_credit['debt_to_income_ratio']

# 查看处理后的数据集
print("\n处理后的数据集前5行:")
print(df_credit.head())
print("\n处理后的数据集形状:", df_credit.shape)

# 划分特征和标签
X = df_credit.drop('default', axis=1)
y = df_credit['default']

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 数据标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\n训练集形状:", X_train_scaled.shape)
print("测试集形状:", X_test_scaled.shape)
```

#### 4.2.4 模型构建与评估

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.pipeline import Pipeline

# 1. 训练多个分类模型
models = {
    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced'),
    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'AdaBoost': AdaBoostClassifier(random_state=42),
    'SVM': SVC(probability=True, random_state=42, class_weight='balanced'),
    'KNN': KNeighborsClassifier()
}

# 评估模型性能
results = {}
for name, model in models.items():
    print(f"\n训练模型: {name}")
    
    # 训练模型
    model.fit(X_train_scaled, y_train)
    
    # 在测试集上进行预测
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test_scaled)
    
    # 计算评估指标
    report = classification_report(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
    pr_auc = average_precision_score(y_test, y_pred_proba)
    
    results[name] = {
        'model': model,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba,
        'report': report,
        'confusion_matrix': cm,
        'roc_auc': roc_auc,
        'precision': precision,
        'recall': recall,
        'pr_auc': pr_auc
    }
    
    print(f"{name} ROC AUC: {roc_auc:.4f}")
    print(f"{name} PR AUC: {pr_auc:.4f}")

# 2. 模型性能比较
# 绘制ROC曲线
plt.figure(figsize=(10, 8))
for name, result in results.items():
    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])
    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {result['roc_auc']:.3f})')
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# 绘制PR曲线
plt.figure(figsize=(10, 8))
for name, result in results.items():
    plt.plot(result['recall'], result['precision'], lw=2, label=f'{name} (AUC = {result['pr_auc']:.3f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall (PR) Curve')
plt.legend(loc="lower left")
plt.show()

# 3. 选择最佳模型进行超参数调优
best_model_name = max(results, key=lambda x: results[x]['roc_auc'])
best_model = results[best_model_name]['model']
print(f"\n最佳模型: {best_model_name}")
print(f"最佳模型ROC AUC: {results[best_model_name]['roc_auc']:.4f}")

# 超参数调优
if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2', None]
    }
    
    grid_search = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'),
                              param_grid=param_grid,
                              cv=StratifiedKFold(n_splits=5),
                              scoring='roc_auc',
                              n_jobs=-1,
                              verbose=1)
    
    grid_search.fit(X_train_scaled, y_train)
    
    print(f"\n最佳参数: {grid_search.best_params_}")
    print(f"最佳交叉验证AUC分数: {grid_search.best_score_:.4f}")
    
    # 使用最佳参数的模型进行最终预测
best_tuned_model = grid_search.best_estimator_
y_pred_tuned = best_tuned_model.predict(X_test_scaled)
y_pred_proba_tuned = best_tuned_model.predict_proba(X_test_scaled)[:, 1]
    
    # 校准概率（对于某些模型可能需要）
    calibrated_model = CalibratedClassifierCV(best_tuned_model, method='sigmoid', cv='prefit')
    calibrated_model.fit(X_train_scaled, y_train)
    y_pred_proba_calibrated = calibrated_model.predict_proba(X_test_scaled)[:, 1]
    
    # 评估调优后的模型
    tuned_roc_auc = roc_auc_score(y_test, y_pred_proba_tuned)
    calibrated_roc_auc = roc_auc_score(y_test, y_pred_proba_calibrated)
    
    print(f"\n调优后模型ROC AUC: {tuned_roc_auc:.4f}")
    print(f"校准后模型ROC AUC: {calibrated_roc_auc:.4f}")
    print("\n调优后模型分类报告:")
    print(classification_report(y_test, y_pred_tuned))

# 4. 特征重要性分析
if hasattr(best_tuned_model, 'feature_importances_'):
    feature_importance = best_tuned_model.feature_importances_
    feature_importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print("\n特征重要性前10名:")
    print(feature_importance_df.head(10))
    
    # 可视化特征重要性
    plt.figure(figsize=(12, 8))
    plt.barh(feature_importance_df['feature'][:10][::-1], feature_importance_df['importance'][:10][::-1])
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.title('Top 10 Features Importance')
    plt.show()
elif hasattr(best_tuned_model, 'coef_'):
    # 对于线性模型，显示系数
    coef_df = pd.DataFrame({
        'feature': X.columns,
        'coefficient': best_tuned_model.coef_[0]
    }).sort_values('coefficient', key=abs, ascending=False)
    
    print("\n特征系数绝对值前10名:")
    print(coef_df.head(10))
    
    # 可视化系数
    plt.figure(figsize=(12, 8))
    top_coef = coef_df.head(10)
    plt.barh(top_coef['feature'], top_coef['coefficient'])
    plt.xlabel('Coefficient Value')
    plt.ylabel('Feature')
    plt.title('Top 10 Features Coefficients')
    plt.axvline(x=0, color='red', linestyle='--')
    plt.show()

# 5. 构建信用评分体系
# 基于模型预测的概率计算信用评分
# 这里使用简化的评分公式：评分 = 基础分 + 概率转换分数

def prob_to_score(prob, base_score=600, score_range=(300, 850)):
    # 将概率转换为评分，概率越低，评分越高
    min_score, max_score = score_range
    # 使用sigmoid函数的反函数将概率映射到评分范围
    log_odds = np.log(prob / (1 - prob + 1e-10))  # 添加小值避免除零错误
    # 假设log_odds的范围是[-10, 10]，将其映射到评分范围
    scaled_score = (log_odds + 10) / 20 * (max_score - min_score) + min_score
    # 确保评分在范围内
    return max(min(scaled_score, max_score), min_score)

# 计算所有测试样本的信用评分
test_scores = [prob_to_score(p) for p in y_pred_proba_calibrated]

# 查看评分分布
plt.figure(figsize=(10, 6))
plt.hist(test_scores, bins=20, alpha=0.7, color='skyblue')
plt.xlabel('Credit Score')
plt.ylabel('Frequency')
plt.title('Credit Score Distribution')
plt.grid(True)
plt.show()

# 根据评分划分风险等级
def score_to_risk_level(score):
    if score >= 700:
        return '低风险'
    elif score >= 600:
        return '中低风险'
    elif score >= 500:
        return '中等风险'
    elif score >= 400:
        return '中高风险'
    else:
        return '高风险'

# 计算每个风险等级的违约率
test_df = pd.DataFrame({
    'score': test_scores,
    'default': y_test
})

test_df['risk_level'] = test_df['score'].apply(score_to_risk_level)

risk_level_stats = test_df.groupby('risk_level').agg({
    'default': ['count', 'mean'],
    'score': ['mean', 'min', 'max']
}).round(2)

print("\n各风险等级统计信息:")
print(risk_level_stats)
```

#### 4.2.5 业务洞察与建议

基于以上分析，我们得出以下业务洞察和建议：

1. **信用评分体系**：建立基于机器学习模型的信用评分体系，可以更准确地评估客户的信用风险，辅助贷款审批决策。

2. **风险定价**：根据客户的信用评分和风险等级，实施差异化的贷款利率定价策略，对于低风险客户可以提供更优惠的利率，对于高风险客户则提高利率或拒绝贷款。

3. **风险管理**：定期监控客户的信用状况变化，对于信用评分下降的客户及时采取风险控制措施。

4. **产品设计**：根据不同风险等级客户的特征，设计符合其需求的金融产品，例如对于中低风险客户可以提供额度更高、期限更长的贷款产品。

5. **营销优化**：针对不同风险等级的客户，制定差异化的营销策略，例如对于高风险客户可以提供信用教育和财务规划服务，帮助其改善信用状况。

## 第五章 总结与展望

### 5.1 主要收获

通过本文的学习，我们掌握了以下关键内容：

1. **高级统计分析方法**：包括高级描述性统计、推断性统计分析和时间序列分析，可以帮助我们更深入地理解数据的分布和特征，发现数据中的规律和趋势。

2. **机器学习算法在数据分析中的应用**：包括监督学习（分类和回归）、无监督学习（聚类和降维）算法的原理、实现和应用场景，可以帮助我们从复杂的数据中提取有价值的信息，进行预测和决策。

3. **特征工程与模型优化技术**：包括特征创建、选择和转换，以及超参数调优、模型集成和避免过拟合等技术，可以显著提高模型的性能和泛化能力。

4. **实战案例分析**：通过电商用户行为分析与推荐系统、金融风险预测与信用评分两个实战案例，展示了如何将理论知识应用到实际业务中，解决实际问题。

### 5.2 最佳实践建议

基于本文的学习和实践经验，我们提出以下最佳实践建议：

1. **数据质量是关键**：在进行数据分析和建模之前，一定要确保数据的质量，包括数据的完整性、准确性和一致性。数据预处理和特征工程是提高模型性能的重要步骤。

2. **选择合适的算法**：根据问题的类型和数据的特点，选择合适的算法。没有最好的算法，只有最适合的算法。对于复杂的问题，可以尝试多种算法并进行比较。

3. **注重模型的可解释性**：在实际业务中，模型的可解释性往往和准确性同样重要。了解模型的决策过程，可以帮助业务人员更好地理解和应用模型的结果。

4. **持续评估和优化**：模型不是一成不变的，需要定期进行评估和优化，以适应数据和业务环境的变化。建立模型监控和反馈机制，及时发现和解决问题。

5. **结合业务知识**：数据分析和机器学习技术应该与业务知识相结合，才能发挥最大的价值。了解业务需求和业务流程，可以帮助我们更好地定义问题、选择特征和解释结果。

### 5.3 未来发展趋势

随着技术的不断发展，高级数据分析和机器学习领域也在不断演进，未来的发展趋势包括：

1. **自动化机器学习（AutoML）**：AutoML技术可以自动完成特征工程、模型选择和超参数调优等任务，降低机器学习的使用门槛，提高开发效率。

2. **深度学习与传统方法的结合**：深度学习在处理图像、文本和语音等非结构化数据方面具有优势，未来将看到更多深度学习与传统统计方法和机器学习算法的结合应用。

3. **联邦学习**：联邦学习可以在不共享原始数据的情况下进行模型训练，保护数据隐私，将在金融、医疗等对数据隐私要求较高的领域得到广泛应用。

4. **可解释人工智能（XAI）**：随着AI应用的普及，对模型可解释性的需求也在增加，XAI技术将帮助用户更好地理解和信任AI系统。

5. **边缘计算与AI的结合**：边缘计算将AI能力扩展到网络边缘，减少数据传输延迟，提高实时处理能力，将在物联网、自动驾驶等领域发挥重要作用。

总之，高级数据分析和机器学习技术正在深刻改变我们的工作和生活方式，掌握这些技术，将帮助我们在大数据时代获得竞争优势。通过不断学习和实践，我们可以更好地利用这些技术解决实际问题，创造更大的价值。